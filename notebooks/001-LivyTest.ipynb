{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext sparkmagic.magics\n",
    "\n",
    "#Livy endpoint http://10.0.123.114:8998\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd1a8ca833a40ad985538c65c638fc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MagicsControllerWidget(children=(Tab(children=(ManageSessionWidget(children=(HTML(value='<br/>'), HTML(value='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%manage_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "::\n",
       "\n",
       "  %spark [-c CONTEXT] [-s SESSION] [-o OUTPUT] [-q [QUIET]]\n",
       "             [-m SAMPLEMETHOD] [-n MAXROWS] [-r SAMPLEFRACTION] [-u URL]\n",
       "             [-a USER] [-p PASSWORD] [-t AUTH] [-l LANGUAGE] [-k [SKIP]]\n",
       "             [-i ID] [-e COERCE]\n",
       "             [command [command ...]]\n",
       "\n",
       "Magic to execute spark remotely.\n",
       "\n",
       "This magic allows you to create a Livy Scala or Python session against a Livy endpoint. Every session can\n",
       "be used to execute either Spark code or SparkSQL code by executing against the SQL context in the session.\n",
       "When the SQL context is used, the result will be a Pandas dataframe of a sample of the results.\n",
       "\n",
       "If invoked with no subcommand, the cell will be executed against the specified session.\n",
       "\n",
       "Subcommands\n",
       "-----------\n",
       "info\n",
       "    Display the available Livy sessions and other configurations for sessions.\n",
       "add\n",
       "    Add a Livy session given a session name (-s), language (-l), and endpoint credentials.\n",
       "    The -k argument, if present, will skip adding this session if it already exists.\n",
       "    e.g. `%spark add -s test -l python -u https://sparkcluster.net/livy -t Kerberos -a u -p -k`\n",
       "config\n",
       "    Override the livy session properties sent to Livy on session creation. All session creations will\n",
       "    contain these config settings from then on.\n",
       "    Expected value is a JSON key-value string to be sent as part of the Request Body for the POST /sessions\n",
       "    endpoint in Livy.\n",
       "    e.g. `%%spark config`\n",
       "         `{\"driverMemory\":\"1000M\", \"executorCores\":4}`\n",
       "run\n",
       "    Run Spark code against a session.\n",
       "    e.g. `%%spark -s testsession` will execute the cell code against the testsession previously created\n",
       "    e.g. `%%spark -s testsession -c sql` will execute the SQL code against the testsession previously created\n",
       "    e.g. `%%spark -s testsession -c sql -o my_var` will execute the SQL code against the testsession\n",
       "             previously created and store the pandas dataframe created in the my_var variable in the\n",
       "             Python environment.\n",
       "logs\n",
       "    Returns the logs for a given session.\n",
       "    e.g. `%spark logs -s testsession` will return the logs for the testsession previously created\n",
       "delete\n",
       "    Delete a Livy session.\n",
       "    e.g. `%spark delete -s defaultlivy`\n",
       "cleanup\n",
       "    Delete all Livy sessions created by the notebook. No arguments required.\n",
       "    e.g. `%spark cleanup`\n",
       "\n",
       "positional arguments:\n",
       "  command               Commands to execute.\n",
       "\n",
       "optional arguments:\n",
       "  -c CONTEXT, --context CONTEXT\n",
       "                        Context to use: 'spark' for spark and 'sql' for sql\n",
       "                        queries. Default is 'spark'.\n",
       "  -s SESSION, --session SESSION\n",
       "                        The name of the Livy session to use.\n",
       "  -o OUTPUT, --output OUTPUT\n",
       "                        If present, output when using SQL queries will be\n",
       "                        stored in this variable.\n",
       "  -q <[QUIET]>, --quiet <[QUIET]>\n",
       "                        Do not display visualizations on SQL queries\n",
       "  -m SAMPLEMETHOD, --samplemethod SAMPLEMETHOD\n",
       "                        Sample method for SQL queries: either take or sample\n",
       "  -n MAXROWS, --maxrows MAXROWS\n",
       "                        Maximum number of rows that will be pulled back from\n",
       "                        the server for SQL queries\n",
       "  -r SAMPLEFRACTION, --samplefraction SAMPLEFRACTION\n",
       "                        Sample fraction for sampling from SQL queries\n",
       "  -u URL, --url URL     URL for Livy endpoint\n",
       "  -a USER, --user USER  Username for HTTP access to Livy endpoint\n",
       "  -p PASSWORD, --password PASSWORD\n",
       "                        Password for HTTP access to Livy endpoint\n",
       "  -t AUTH, --auth AUTH  Auth type for HTTP access to Livy endpoint. [Kerberos,\n",
       "                        None, Basic]\n",
       "  -l LANGUAGE, --language LANGUAGE\n",
       "                        Language for Livy session; one of python, scala, r\n",
       "  -k <[SKIP]>, --skip <[SKIP]>\n",
       "                        Skip adding session if it already exists\n",
       "  -i ID, --id ID        Session ID\n",
       "  -e COERCE, --coerce COERCE\n",
       "                        Whether to automatically coerce the types (default,\n",
       "                        pass True if being explicit) of the dataframe or not\n",
       "                        (pass False)\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/conda/lib/python3.8/site-packages/sparkmagic/livyclientlib/exceptions.py\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%spark?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First element of numbers is 1 and its description is:\n",
      "b'(2) ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274 []'\n",
      "Count of numbers is 4 \n",
      "b'(2) ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274 []'"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "numbers = sc.parallelize([1, 2, 3, 4])\n",
    "print('First element of numbers is {} and its description is:\\n{}'.format(numbers.first(), numbers.toDebugString()))\n",
    "print('Count of numbers is {} \\n{}'.format(numbers.count(), numbers.toDebugString()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info for running Spark:\n",
      "    Sessions:\n",
      "        Name: spark-session\tSession id: 3\tYARN id: application_1628875993215_0027\tKind: pyspark\tState: idle\n",
      "\tSpark UI: http://ip-10-0-123-114.us-west-2.compute.internal:20888/proxy/application_1628875993215_0027/\n",
      "\tDriver Log: http://ip-10-0-113-154.us-west-2.compute.internal:8042/node/containerlogs/container_1628875993215_0027_01_000001/dominospark\n",
      "    Session configs:\n",
      "        {'driverMemory': '1000M', 'executorCores': 2}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%spark info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark -s spark-session\n",
    "##Only if you have multiple spark sessions or else simple %%spark works\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, DoubleType, IntegerType\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|name|value|\n",
      "+----+-----+\n",
      "|  Pi|3.096|\n",
      "+----+-----+"
     ]
    }
   ],
   "source": [
    "%%spark -s spark-session\n",
    "##Only if you have multiple spark sessions or else simple %%spark works\n",
    "\n",
    "def inside(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "columns = StructType([ StructField(\"name\", StringType(), True),\n",
    "                      StructField(\"value\", DoubleType(), True)\n",
    "                    ])\n",
    "\n",
    "count = sc.parallelize(range(0, 1000),1) \\\n",
    "             .filter(inside).count()\n",
    "data = [(\"Pi\",4.0 * count/1000)]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "| id| v1| v2| v3|\n",
      "+---+---+---+---+\n",
      "|500|  2| 41| 37|\n",
      "|501| 59| 45| 92|\n",
      "|502| 45| 21| 57|\n",
      "|503| 81| 65| 86|\n",
      "|504| 14|  3| 98|\n",
      "|505| 22| 78|  9|\n",
      "|506| 69| 15| 29|\n",
      "|507| 95| 24| 10|\n",
      "|508| 53| 50| 28|\n",
      "|509| 84| 69| 78|\n",
      "|510| 89| 26| 12|\n",
      "|511| 88| 19|  6|\n",
      "|512| 96| 87| 16|\n",
      "|513| 73| 51| 95|\n",
      "|514| 90|  2| 64|\n",
      "|515| 67| 27| 94|\n",
      "|516| 96| 47| 85|\n",
      "|517| 33| 11| 46|\n",
      "|518| 17| 48| 85|\n",
      "|519| 87| 53| 32|\n",
      "+---+---+---+---+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "hdfs_path='/user/dominospark/large-data-livy-2'\n",
    "data = []\n",
    "for i in range(1000):\n",
    "    data.append((i,random.randint(1,100),random.randint(1,100),random.randint(1,100)))\n",
    "\n",
    "columns = StructType([ StructField(\"id\", IntegerType(), True),\n",
    "                       StructField(\"v1\", IntegerType(), True),\n",
    "                       StructField(\"v2\", IntegerType(), True),\n",
    "                       StructField(\"v3\", IntegerType(), True) ])\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.write.csv(hdfs_path)\n",
    "\n",
    "df_load = spark.read.csv( hdfs_path, schema=columns)\n",
    "df_load.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stdout: \n",
      "\n",
      "stderr: \n",
      "21/08/17 01:25:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "21/08/17 01:25:14 INFO RMProxy: Connecting to ResourceManager at ip-10-0-123-114.us-west-2.compute.internal/10.0.123.114:8032\n",
      "21/08/17 01:25:14 INFO Client: Requesting a new application from cluster with 2 NodeManagers\n",
      "21/08/17 01:25:15 INFO Configuration: resource-types.xml not found\n",
      "21/08/17 01:25:15 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "21/08/17 01:25:15 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)\n",
      "21/08/17 01:25:15 INFO Client: Will allocate AM container, with 1384 MB memory including 384 MB overhead\n",
      "21/08/17 01:25:15 INFO Client: Setting up container launch context for our AM\n",
      "21/08/17 01:25:15 INFO Client: Setting up the launch environment for our AM container\n",
      "21/08/17 01:25:15 INFO Client: Preparing resources for our AM container\n",
      "21/08/17 01:25:15 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "21/08/17 01:25:17 INFO Client: Uploading resource file:/mnt/tmp/spark-d82e8c52-fc8b-47b9-90f4-9874e803f320/__spark_libs__2203053601780232499.zip -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0027/__spark_libs__2203053601780232499.zip\n",
      "21/08/17 01:25:18 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-api-0.7.0-incubating.jar -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0027/livy-api-0.7.0-incubating.jar\n",
      "21/08/17 01:25:18 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-rsc-0.7.0-incubating.jar -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0027/livy-rsc-0.7.0-incubating.jar\n",
      "21/08/17 01:25:18 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-thriftserver-session-0.7.0-incubating.jar -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0027/livy-thriftserver-session-0.7.0-incubating.jar\n",
      "21/08/17 01:25:18 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0027/netty-all-4.1.17.Final.jar\n",
      "21/08/17 01:25:18 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/commons-codec-1.9.jar -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0027/commons-codec-1.9.jar\n",
      "21/08/17 01:25:18 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/livy-core_2.12-0.7.0-incubating.jar -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0027/livy-core_2.12-0.7.0-incubating.jar\n",
      "21/08/17 01:25:18 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/livy-repl_2.12-0.7.0-incubating.jar -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0027/livy-repl_2.12-0.7.0-incubating.jar\n",
      "21/08/17 01:25:18 INFO Client: Uploading resource file:/etc/spark/conf/hive-site.xml -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0027/hive-site.xml\n",
      "21/08/17 01:25:18 INFO Client: Uploading resource file:/usr/lib/spark/R/lib/sparkr.zip#sparkr -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0027/sparkr.zip\n",
      "21/08/17 01:25:18 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0027/pyspark.zip\n",
      "21/08/17 01:25:18 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9-src.zip -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0027/py4j-0.10.9-src.zip\n",
      "21/08/17 01:25:18 INFO Client: Uploading resource file:/mnt/tmp/spark-d82e8c52-fc8b-47b9-90f4-9874e803f320/__spark_conf__4678917456317088679.zip -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0027/__spark_conf__.zip\n",
      "21/08/17 01:25:18 INFO SecurityManager: Changing view acls to: livy,dominospark\n",
      "21/08/17 01:25:18 INFO SecurityManager: Changing modify acls to: livy,dominospark\n",
      "21/08/17 01:25:18 INFO SecurityManager: Changing view acls groups to: \n",
      "21/08/17 01:25:18 INFO SecurityManager: Changing modify acls groups to: \n",
      "21/08/17 01:25:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, dominospark); groups with view permissions: Set(); users  with modify permissions: Set(livy, dominospark); groups with modify permissions: Set()\n",
      "21/08/17 01:25:18 INFO HadoopFSDelegationTokenProvider: getting token for: DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_407257000_1, ugi=dominospark (auth:PROXY) via livy/ip-10-0-123-114.us-west-2.compute.internal@KDCDOMINO.COM (auth:KERBEROS)]] with renewer yarn/ip-10-0-123-114.us-west-2.compute.internal@KDCDOMINO.COM\n",
      "21/08/17 01:25:18 INFO DFSClient: Created token for dominospark: HDFS_DELEGATION_TOKEN owner=dominospark, renewer=yarn, realUser=livy/ip-10-0-123-114.us-west-2.compute.internal@KDCDOMINO.COM, issueDate=1629163518549, maxDate=1629768318549, sequenceNumber=75, masterKeyId=5 on 10.0.123.114:8020\n",
      "21/08/17 01:25:18 INFO KMSClientProvider: New token created: (Kind: kms-dt, Service: kms://http@ip-10-0-123-114.us-west-2.compute.internal:9600/kms, Ident: (kms-dt owner=dominospark, renewer=yarn, realUser=livy, issueDate=1629163518608, maxDate=1629768318608, sequenceNumber=48, masterKeyId=5))\n",
      "21/08/17 01:25:18 INFO HadoopFSDelegationTokenProvider: getting token for: DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_407257000_1, ugi=dominospark (auth:PROXY) via livy/ip-10-0-123-114.us-west-2.compute.internal@KDCDOMINO.COM (auth:KERBEROS)]] with renewer dominospark\n",
      "21/08/17 01:25:18 INFO DFSClient: Created token for dominospark: HDFS_DELEGATION_TOKEN owner=dominospark, renewer=dominospark, realUser=livy/ip-10-0-123-114.us-west-2.compute.internal@KDCDOMINO.COM, issueDate=1629163518794, maxDate=1629768318794, sequenceNumber=76, masterKeyId=5 on 10.0.123.114:8020\n",
      "21/08/17 01:25:18 INFO KMSClientProvider: New token created: (Kind: kms-dt, Service: kms://http@ip-10-0-123-114.us-west-2.compute.internal:9600/kms, Ident: (kms-dt owner=dominospark, renewer=dominospark, realUser=livy, issueDate=1629163518812, maxDate=1629768318812, sequenceNumber=49, masterKeyId=5))\n",
      "21/08/17 01:25:18 INFO HadoopFSDelegationTokenProvider: Renewal interval is 86400026 for token kms-dt\n",
      "21/08/17 01:25:18 INFO HadoopFSDelegationTokenProvider: Renewal interval is 86400051 for token HDFS_DELEGATION_TOKEN\n",
      "21/08/17 01:25:19 INFO HiveConf: Found configuration file file:/etc/spark/conf.dist/hive-site.xml\n",
      "21/08/17 01:25:19 WARN HiveConf: HiveConf of name hive.server2.thrift.url does not exist\n",
      "21/08/17 01:25:20 WARN HiveConf: HiveConf of name hive.server2.thrift.url does not exist\n",
      "21/08/17 01:25:20 INFO metastore: Trying to connect to metastore with URI thrift://ip-10-0-123-114.us-west-2.compute.internal:9083\n",
      "21/08/17 01:25:20 INFO metastore: Opened a connection to metastore, current connections: 1\n",
      "21/08/17 01:25:20 INFO metastore: Connected to metastore.\n",
      "21/08/17 01:25:20 INFO metastore: Closed a connection to metastore, current connections: 0\n",
      "21/08/17 01:25:20 INFO Client: Submitting application application_1628875993215_0027 to ResourceManager\n",
      "21/08/17 01:25:20 INFO YarnClientImpl: Submitted application application_1628875993215_0027\n",
      "21/08/17 01:25:20 INFO Client: Application report for application_1628875993215_0027 (state: ACCEPTED)\n",
      "21/08/17 01:25:20 INFO Client: \n",
      "\t client token: Token { kind: YARN_CLIENT_TOKEN, service:  }\n",
      "\t diagnostics: AM container is launched, waiting for AM container to Register with RM\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1629163520662\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-10-0-123-114.us-west-2.compute.internal:20888/proxy/application_1628875993215_0027/\n",
      "\t user: dominospark\n",
      "21/08/17 01:25:20 INFO ShutdownHookManager: Shutdown hook called\n",
      "21/08/17 01:25:20 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-8b63d5d1-c4c0-4e4d-93c2-0ab2e96df92f\n",
      "21/08/17 01:25:20 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-d82e8c52-fc8b-47b9-90f4-9874e803f320\n",
      "\n",
      "YARN Diagnostics: "
     ]
    }
   ],
   "source": [
    "%spark logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd1a8ca833a40ad985538c65c638fc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MagicsControllerWidget(children=(Tab(children=(ManageSessionWidget(children=(HTML(value='<br/>'), HBox(childre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "%manage_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
