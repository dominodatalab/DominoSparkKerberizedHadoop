{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext sparkmagic.magics\n",
    "\n",
    "#Livy endpoint http://10.0.123.114:8998\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd1a8ca833a40ad985538c65c638fc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MagicsControllerWidget(children=(Tab(children=(ManageSessionWidget(children=(HTML(value='<br/>'), HTML(value='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%manage_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First element of numbers is 1 and its description is:\n",
      "b'(2) ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274 []'\n",
      "Count of numbers is 4 \n",
      "b'(2) ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274 []'"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "numbers = sc.parallelize([1, 2, 3, 4])\n",
    "print('First element of numbers is {} and its description is:\\n{}'.format(numbers.first(), numbers.toDebugString()))\n",
    "print('Count of numbers is {} \\n{}'.format(numbers.count(), numbers.toDebugString()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info for running Spark:\n",
      "    Sessions:\n",
      "        Name: spark-session\tSession id: 2\tYARN id: application_1628875993215_0026\tKind: pyspark\tState: idle\n",
      "\tSpark UI: http://ip-10-0-123-114.us-west-2.compute.internal:20888/proxy/application_1628875993215_0026/\n",
      "\tDriver Log: http://ip-10-0-96-11.us-west-2.compute.internal:8042/node/containerlogs/container_1628875993215_0026_01_000001/dominospark\n",
      "    Session configs:\n",
      "        {'driverMemory': '1000M', 'executorCores': 2}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%spark info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark -s spark-session\n",
    "##Only if you have multiple spark sessions or else simple %%spark works\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, DoubleType, IntegerType\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|name|value|\n",
      "+----+-----+\n",
      "|  Pi|3.052|\n",
      "+----+-----+"
     ]
    }
   ],
   "source": [
    "%%spark -s spark-session\n",
    "##Only if you have multiple spark sessions or else simple %%spark works\n",
    "\n",
    "def inside(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "columns = StructType([ StructField(\"name\", StringType(), True),\n",
    "                      StructField(\"value\", DoubleType(), True)\n",
    "                    ])\n",
    "\n",
    "count = sc.parallelize(range(0, 1000),1) \\\n",
    "             .filter(inside).count()\n",
    "data = [(\"Pi\",4.0 * count/1000)]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+---+\n",
      "|   id| v1| v2| v3|\n",
      "+-----+---+---+---+\n",
      "|50176| 63| 56| 12|\n",
      "|50177| 16| 58| 87|\n",
      "|50178| 58| 88| 79|\n",
      "|50179| 21| 12| 82|\n",
      "|50180| 73| 13| 77|\n",
      "|50181| 16| 96| 27|\n",
      "|50182| 10| 35| 79|\n",
      "|50183| 12| 65| 33|\n",
      "|50184| 11| 43| 57|\n",
      "|50185| 25| 99| 78|\n",
      "|50186| 12| 97| 81|\n",
      "|50187| 66| 11|  2|\n",
      "|50188| 79| 65|  9|\n",
      "|50189| 25| 38| 40|\n",
      "|50190| 48|  8| 86|\n",
      "|50191| 86| 97| 49|\n",
      "|50192| 38| 58| 27|\n",
      "|50193|  4| 91| 48|\n",
      "|50194| 67| 52| 28|\n",
      "|50195| 59| 23| 58|\n",
      "+-----+---+---+---+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "hdfs_path='/user/dominospark/large-data-livy'\n",
    "data = []\n",
    "for i in range(100000):\n",
    "    data.append((i,random.randint(1,100),random.randint(1,100),random.randint(1,100)))\n",
    "\n",
    "columns = StructType([ StructField(\"id\", IntegerType(), True),\n",
    "                       StructField(\"v1\", IntegerType(), True),\n",
    "                       StructField(\"v2\", IntegerType(), True),\n",
    "                       StructField(\"v3\", IntegerType(), True) ])\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.write.csv(hdfs_path)\n",
    "\n",
    "df_load = spark.read.csv( hdfs_path, schema=columns)\n",
    "df_load.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stdout: \n",
      "\n",
      "stderr: \n",
      "21/08/16 21:02:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "21/08/16 21:02:08 INFO RMProxy: Connecting to ResourceManager at ip-10-0-123-114.us-west-2.compute.internal/10.0.123.114:8032\n",
      "21/08/16 21:02:08 INFO Client: Requesting a new application from cluster with 2 NodeManagers\n",
      "21/08/16 21:02:09 INFO Configuration: resource-types.xml not found\n",
      "21/08/16 21:02:09 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "21/08/16 21:02:09 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)\n",
      "21/08/16 21:02:09 INFO Client: Will allocate AM container, with 1384 MB memory including 384 MB overhead\n",
      "21/08/16 21:02:09 INFO Client: Setting up container launch context for our AM\n",
      "21/08/16 21:02:09 INFO Client: Setting up the launch environment for our AM container\n",
      "21/08/16 21:02:09 INFO Client: Preparing resources for our AM container\n",
      "21/08/16 21:02:09 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "21/08/16 21:02:11 INFO Client: Uploading resource file:/mnt/tmp/spark-a9716f15-9465-41d7-9082-df3946058e14/__spark_libs__3084380948426205107.zip -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0026/__spark_libs__3084380948426205107.zip\n",
      "21/08/16 21:02:12 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-api-0.7.0-incubating.jar -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0026/livy-api-0.7.0-incubating.jar\n",
      "21/08/16 21:02:12 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-rsc-0.7.0-incubating.jar -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0026/livy-rsc-0.7.0-incubating.jar\n",
      "21/08/16 21:02:12 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-thriftserver-session-0.7.0-incubating.jar -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0026/livy-thriftserver-session-0.7.0-incubating.jar\n",
      "21/08/16 21:02:12 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0026/netty-all-4.1.17.Final.jar\n",
      "21/08/16 21:02:12 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/commons-codec-1.9.jar -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0026/commons-codec-1.9.jar\n",
      "21/08/16 21:02:12 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/livy-core_2.12-0.7.0-incubating.jar -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0026/livy-core_2.12-0.7.0-incubating.jar\n",
      "21/08/16 21:02:12 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/livy-repl_2.12-0.7.0-incubating.jar -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0026/livy-repl_2.12-0.7.0-incubating.jar\n",
      "21/08/16 21:02:12 INFO Client: Uploading resource file:/etc/spark/conf/hive-site.xml -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0026/hive-site.xml\n",
      "21/08/16 21:02:12 INFO Client: Uploading resource file:/usr/lib/spark/R/lib/sparkr.zip#sparkr -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0026/sparkr.zip\n",
      "21/08/16 21:02:12 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0026/pyspark.zip\n",
      "21/08/16 21:02:12 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9-src.zip -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0026/py4j-0.10.9-src.zip\n",
      "21/08/16 21:02:12 INFO Client: Uploading resource file:/mnt/tmp/spark-a9716f15-9465-41d7-9082-df3946058e14/__spark_conf__8813725059694125550.zip -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0026/__spark_conf__.zip\n",
      "21/08/16 21:02:12 INFO SecurityManager: Changing view acls to: livy,dominospark\n",
      "21/08/16 21:02:12 INFO SecurityManager: Changing modify acls to: livy,dominospark\n",
      "21/08/16 21:02:12 INFO SecurityManager: Changing view acls groups to: \n",
      "21/08/16 21:02:12 INFO SecurityManager: Changing modify acls groups to: \n",
      "21/08/16 21:02:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, dominospark); groups with view permissions: Set(); users  with modify permissions: Set(livy, dominospark); groups with modify permissions: Set()\n",
      "21/08/16 21:02:12 INFO HadoopFSDelegationTokenProvider: getting token for: DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_1083605865_1, ugi=dominospark (auth:PROXY) via livy/ip-10-0-123-114.us-west-2.compute.internal@KDCDOMINO.COM (auth:KERBEROS)]] with renewer yarn/ip-10-0-123-114.us-west-2.compute.internal@KDCDOMINO.COM\n",
      "21/08/16 21:02:12 INFO DFSClient: Created token for dominospark: HDFS_DELEGATION_TOKEN owner=dominospark, renewer=yarn, realUser=livy/ip-10-0-123-114.us-west-2.compute.internal@KDCDOMINO.COM, issueDate=1629147732693, maxDate=1629752532693, sequenceNumber=73, masterKeyId=5 on 10.0.123.114:8020\n",
      "21/08/16 21:02:12 INFO KMSClientProvider: New token created: (Kind: kms-dt, Service: kms://http@ip-10-0-123-114.us-west-2.compute.internal:9600/kms, Ident: (kms-dt owner=dominospark, renewer=yarn, realUser=livy, issueDate=1629147732756, maxDate=1629752532756, sequenceNumber=46, masterKeyId=5))\n",
      "21/08/16 21:02:12 INFO HadoopFSDelegationTokenProvider: getting token for: DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_1083605865_1, ugi=dominospark (auth:PROXY) via livy/ip-10-0-123-114.us-west-2.compute.internal@KDCDOMINO.COM (auth:KERBEROS)]] with renewer dominospark\n",
      "21/08/16 21:02:12 INFO DFSClient: Created token for dominospark: HDFS_DELEGATION_TOKEN owner=dominospark, renewer=dominospark, realUser=livy/ip-10-0-123-114.us-west-2.compute.internal@KDCDOMINO.COM, issueDate=1629147732941, maxDate=1629752532941, sequenceNumber=74, masterKeyId=5 on 10.0.123.114:8020\n",
      "21/08/16 21:02:12 INFO KMSClientProvider: New token created: (Kind: kms-dt, Service: kms://http@ip-10-0-123-114.us-west-2.compute.internal:9600/kms, Ident: (kms-dt owner=dominospark, renewer=dominospark, realUser=livy, issueDate=1629147732958, maxDate=1629752532958, sequenceNumber=47, masterKeyId=5))\n",
      "21/08/16 21:02:12 INFO HadoopFSDelegationTokenProvider: Renewal interval is 86400021 for token kms-dt\n",
      "21/08/16 21:02:12 INFO HadoopFSDelegationTokenProvider: Renewal interval is 86400043 for token HDFS_DELEGATION_TOKEN\n",
      "21/08/16 21:02:14 INFO HiveConf: Found configuration file file:/etc/spark/conf.dist/hive-site.xml\n",
      "21/08/16 21:02:14 WARN HiveConf: HiveConf of name hive.server2.thrift.url does not exist\n",
      "21/08/16 21:02:14 WARN HiveConf: HiveConf of name hive.server2.thrift.url does not exist\n",
      "21/08/16 21:02:14 INFO metastore: Trying to connect to metastore with URI thrift://ip-10-0-123-114.us-west-2.compute.internal:9083\n",
      "21/08/16 21:02:14 INFO metastore: Opened a connection to metastore, current connections: 1\n",
      "21/08/16 21:02:14 INFO metastore: Connected to metastore.\n",
      "21/08/16 21:02:14 INFO metastore: Closed a connection to metastore, current connections: 0\n",
      "21/08/16 21:02:14 INFO Client: Submitting application application_1628875993215_0026 to ResourceManager\n",
      "21/08/16 21:02:15 INFO YarnClientImpl: Submitted application application_1628875993215_0026\n",
      "21/08/16 21:02:15 INFO Client: Application report for application_1628875993215_0026 (state: ACCEPTED)\n",
      "21/08/16 21:02:15 INFO Client: \n",
      "\t client token: Token { kind: YARN_CLIENT_TOKEN, service:  }\n",
      "\t diagnostics: AM container is launched, waiting for AM container to Register with RM\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1629147734829\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-10-0-123-114.us-west-2.compute.internal:20888/proxy/application_1628875993215_0026/\n",
      "\t user: dominospark\n",
      "21/08/16 21:02:15 INFO ShutdownHookManager: Shutdown hook called\n",
      "21/08/16 21:02:15 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-a9716f15-9465-41d7-9082-df3946058e14\n",
      "21/08/16 21:02:15 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-404005db-95e1-4b69-b365-23600159a252\n",
      "\n",
      "YARN Diagnostics: "
     ]
    }
   ],
   "source": [
    "%spark logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b8a1cd4643147c6828217a0613eb1d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MagicsControllerWidget(children=(Tab(children=(ManageSessionWidget(children=(HTML(value='<br/>'), HBox(childre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "%manage_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
