{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HDFS_PATH=/user/dominospark/small_data\n",
      "env: HADOOP_HOME=/usr/lib/hadoop\n",
      "env: HADOOP_CONF_DIR=/etc/hadoop/conf\n",
      "env: HADOOP_YARN_HOME=/usr/lib/hadoop\n",
      "env: HADOOP_MAPRED_HOME=/usr/lib/hadoop\n",
      "env: HADOOP_HDFS_HOME=/usr/lib/hadoop-hdfs\n",
      "env: SPARK_HOME=/usr/lib/spark\n",
      "env: SPARK_CONF_DIR=/etc/spark/conf\n",
      "env: PYSPARK_PYTHON=/opt/conda/bin/python\n",
      "env: PYSPARK_DRIVER_PYTHON=/opt/conda/bin/python\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "hdfs_endpoint=os.environ['HDFS_ENDPOINT']\n",
    "%env HDFS_PATH /user/dominospark/small_data\n",
    "%env HADOOP_HOME /usr/lib/hadoop\n",
    "%env HADOOP_CONF_DIR /etc/hadoop/conf\n",
    "%env HADOOP_YARN_HOME /usr/lib/hadoop\n",
    "%env HADOOP_MAPRED_HOME /usr/lib/hadoop\n",
    "%env HADOOP_HDFS_HOME /usr/lib/hadoop-hdfs\n",
    "\n",
    "%env SPARK_HOME /usr/lib/spark\n",
    "%env SPARK_CONF_DIR /etc/spark/conf\n",
    "#%env PYTHONPATH /opt/spark/python/lib/py4j-0.10.7-src.zip\n",
    "%env PYSPARK_PYTHON /opt/conda/bin/python\n",
    "%env PYSPARK_DRIVER_PYTHON /opt/conda/bin/python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use '-rm -r' instead.\n",
      "rmr: `/user/dominospark/smalldata-10': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, DoubleType, IntegerType\n",
    "import random\n",
    "sys.path.insert(1, '/mnt/code/python/')\n",
    "count=10\n",
    "\n",
    "src_folder='/user/dominospark/large-data-temp/'\n",
    "dest_folder='/user/dominospark/smalldata-10'\n",
    "\n",
    "!hdfs dfs -rmr '/user/dominospark/smalldata-10'\n",
    "\n",
    "import filter_data_in_hdfs\n",
    "args=[count,hdfs_endpoint,src_folder,dest_folder,'local[*]']\n",
    "filter_data_in_hdfs.main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, DoubleType, IntegerType\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sparkSession = SparkSession.builder.appName(\"example-pyspark-read-and-write\") \\\n",
    ".master('local[*]') \\\n",
    ".config(\"fs.default.name\", hdfs_endpoint) \\\n",
    ".getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "| id| v1| v2| v3|\n",
      "+---+---+---+---+\n",
      "|  0| 50| 57| 20|\n",
      "|  1| 57| 52| 49|\n",
      "|  2|  7| 50| 12|\n",
      "|  3| 60| 65| 30|\n",
      "|  4| 66| 31| 85|\n",
      "|  5| 69| 98| 99|\n",
      "|  6|  4| 28|  7|\n",
      "|  7|  7| 11| 83|\n",
      "|  8| 74| 95| 88|\n",
      "|  9| 20| 34| 38|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "columns = StructType([ StructField(\"id\", IntegerType(), True),\n",
    "                       StructField(\"v1\", IntegerType(), True),\n",
    "                       StructField(\"v2\", IntegerType(), True),\n",
    "                       StructField(\"v3\", IntegerType(), True) ])\n",
    "\n",
    "\n",
    "df = sparkSession.read.csv(dest_folder, schema=columns)\n",
    "df.show()\n",
    "df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "| id| v1| v2| v3|\n",
      "+---+---+---+---+\n",
      "|  0| 50| 57| 20|\n",
      "|  1| 57| 52| 49|\n",
      "|  2|  7| 50| 12|\n",
      "|  3| 60| 65| 30|\n",
      "|  4| 66| 31| 85|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_load = sparkSession.read.csv(dest_folder,columns)\n",
    "df_load.where(df_load.id < 5).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 dominospark dominospark          0 2021-08-13 19:36 /user/dominospark/smalldata-10/_SUCCESS\n",
      "-rw-r--r--   1 dominospark dominospark        106 2021-08-13 19:36 /user/dominospark/smalldata-10/part-00000-ae995e96-dbc3-499f-b699-2507813dc9b7-c000.csv\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls '/user/dominospark/smalldata-10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
