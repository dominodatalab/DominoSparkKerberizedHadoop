{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HADOOP_TOKEN_FILE_LOCATION=/mnt/data/ON-DEMAND-SPARK/hdfsdt.token\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#Generate an HDFS Delegation token\n",
    "%env HADOOP_TOKEN_FILE_LOCATION=/mnt/data/ON-DEMAND-SPARK/hdfsdt.token\n",
    "#Run the following commands\n",
    "#!/mnt/code/scripts/my_hdfs.sh fetchdt --renewer null $HADOOP_TOKEN_FILE_LOCATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYSPARK_PYTHON=/usr/bin/python3\n",
      "env: PYSPARK_DRIVER_PYTHON=/usr/bin/python3\n"
     ]
    }
   ],
   "source": [
    "%env PYSPARK_PYTHON /usr/bin/python3\n",
    "%env PYSPARK_DRIVER_PYTHON /usr/bin/python3\n",
    "#%env SPARK_DIST_CLASSPATH  /etc/hadoop/conf:/opt/hadoop/share/hadoop/common/lib/*:/opt/hadoop/share/hadoop/common/*:/usr/lib/hadoop-lzo/lib/hadoop-lzo-0.4.19.jar:/usr/lib/hadoop-lzo/lib/hadoop-lzo.jar:/usr/lib/hadoop-lzo/lib/native:/usr/share/aws/aws-java-sdk/aws-java-sdk-bundle-1.11.977.jar:/usr/share/aws/aws-java-sdk/LICENSE.txt:/usr/share/aws/aws-java-sdk/NOTICE.txt:/usr/share/aws/aws-java-sdk/README.md:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/annotations-16.0.2.jar:/usr/share/aws/emr/emrfs/lib/aopalliance-1.0.jar:/usr/share/aws/emr/emrfs/lib/aws-glue-sdk-1.12.0.jar:/usr/share/aws/emr/emrfs/lib/bcprov-ext-jdk15on-1.66.jar:/usr/share/aws/emr/emrfs/lib/emrfs-hadoop-assembly-2.46.0.jar:/usr/share/aws/emr/emrfs/lib/fluent-hc-4.5.6.jar:/usr/share/aws/emr/emrfs/lib/ion-java-1.0.2.jar:/usr/share/aws/emr/emrfs/lib/javax.inject-1.jar:/usr/share/aws/emr/emrfs/lib/jcl-over-slf4j-1.7.21.jar:/usr/share/aws/emr/emrfs/lib/jmespath-java-1.11.852.jar:/usr/share/aws/emr/emrfs/lib/lombok-1.18.4.jar:/usr/share/aws/emr/emrfs/lib/mockito-core-1.10.19.jar:/usr/share/aws/emr/emrfs/lib/objenesis-2.1.jar:/usr/share/aws/emr/emrfs/lib/secret-agent-interface-1.4.0.jar:/usr/share/aws/emr/emrfs/lib/slf4j-api-1.7.21.jar:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/ddb/lib/emr-ddb-hadoop.jar:/usr/share/aws/emr/goodies/lib/emr-hadoop-goodies.jar:/usr/share/aws/emr/kinesis/lib/emr-kinesis-hadoop.jar:/usr/share/aws/emr/cloudwatch-sink/lib/cloudwatch-sink-2.1.0.jar:/usr/share/aws/emr/cloudwatch-sink/lib/cloudwatch-sink.jar:/opt/hadoop/share/hadoop/tools/lib/*:/usr/lib/hadoop-lzo/lib/*\n",
    "#%env JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64\n",
    "hdfs_endpoint=os.environ['HDFS_ENDPOINT']\n",
    "hdfs_endpoint\n",
    "hdfs_dt_token_path=os.environ['HADOOP_TOKEN_FILE_LOCATION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, DoubleType, IntegerType\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession = SparkSession.builder.appName(\"Calculate Pi using OnDemand Spark\") \\\n",
    ".config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    ".config(\"fs.default.name\", hdfs_endpoint) \\\n",
    ".config(\"hadoop.security.authentication\", \"token\")\\\n",
    ".config(\"spark.driver.extraClassPath\", \"/opt/hadoop/etc/hadoop:/usr/lib/hadoop-lzo/lib/*:/opt/hadoop/share/hadoop/mapreduce/*:/opt/hadoop/share/hadoop/hdfs/*\") \\\n",
    ".config(\"spark.executorEnv.HADOOP_TOKEN_FILE_LOCATION\",hdfs_dt_token_path) \\\n",
    ".config(\"spark.driverEnv.HADOOP_TOKEN_FILE_LOCATION\",hdfs_dt_token_path) \\\n",
    ".getOrCreate()\n",
    "sc=sparkSession.sparkContext\n",
    "#.config(\"spark.executor.extraClassPath\", \"/opt/hadoop/etc/hadoop:/usr/lib/hadoop-lzo/lib/*:/opt/hadoop/share/hadoop/mapreduce/*:/opt/hadoop/share/hadoop/hdfs/*\") \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inside(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|name|value|\n",
      "+----+-----+\n",
      "|  Pi| 3.16|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "columns = StructType([ StructField(\"name\", StringType(), True),\n",
    "                      StructField(\"value\", DoubleType(), True)\n",
    "                    ])\n",
    "\n",
    "count = sc.parallelize(range(0, 1000),1) \\\n",
    "             .filter(inside).count()\n",
    "data = [(\"Pi\",4.0 * count/1000)]\n",
    "\n",
    "df = sparkSession.createDataFrame(data=data, schema=columns)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use '-rm -r' instead.\n",
      "rmr: `/user/dominospark/my-pi*': No such file or directory\n",
      "+---+----+\n",
      "|_c0| _c1|\n",
      "+---+----+\n",
      "| Pi|3.16|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Let us write to a dataset\n",
    "ds_path = '/user/dominospark/my-pi'\n",
    "!/mnt/code/scripts/my_hdfs.sh dfs -rmr '/user/dominospark/my-pi*'\n",
    "df.write.csv(ds_path)\n",
    "#Read it back\n",
    "sparkSession.read.csv(ds_path).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_src_path = '/user/dominospark/largedata/'\n",
    "hdfs_dest_path =  '/user/dominospark/small-data-100/'\n",
    "local_dest_path = 'file:///mnt/data/ON-DEMAND-SPARK/small-data-100'\n",
    "filter_criteria = 100\n",
    "sparkSession = SparkSession.builder.appName(\"Generate Data\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    "    .config(\"fs.default.name\", hdfs_endpoint) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "    \n",
    "columns = StructType([ StructField(\"id\", IntegerType(), True), \\\n",
    "                       StructField(\"v1\", IntegerType(), True),\\\n",
    "                       StructField(\"v2\", IntegerType(), True),\\\n",
    "                       StructField(\"v3\", IntegerType(), True) ])\n",
    "\n",
    "df_load = sparkSession.read.csv(hdfs_src_path,columns)\n",
    "df_load_filtered = df_load.where(df_load.id < filter_criteria)\n",
    "df_load_filtered.write.csv(hdfs_dest_path)\n",
    "df_load_filtered.write.csv(local_dest_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "| id| v1| v2| v3|\n",
      "+---+---+---+---+\n",
      "|  0| 63| 94| 50|\n",
      "|  1| 25| 26| 73|\n",
      "|  2| 84| 84| 84|\n",
      "|  3| 47| 19|  8|\n",
      "|  4| 24| 31|  6|\n",
      "|  5| 75| 17| 11|\n",
      "|  6| 49| 38| 57|\n",
      "|  7| 56| 31| 90|\n",
      "|  8|100|  3|  3|\n",
      "|  9| 43| 72| 34|\n",
      "| 10| 18| 64| 57|\n",
      "| 11| 63| 75| 80|\n",
      "| 12| 82| 85| 28|\n",
      "| 13| 31|  8| 42|\n",
      "| 14| 20| 80|  3|\n",
      "| 15| 27| 91| 86|\n",
      "| 16| 55| 70| 42|\n",
      "| 17| 69|  3|  5|\n",
      "| 18| 65| 28| 28|\n",
      "| 19| 57|  8| 69|\n",
      "+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_load_read = sparkSession.read.csv(hdfs_dest_path,columns)\n",
    "df_load_read.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "| id| v1| v2| v3|\n",
      "+---+---+---+---+\n",
      "|  0| 63| 94| 50|\n",
      "|  1| 25| 26| 73|\n",
      "|  2| 84| 84| 84|\n",
      "|  3| 47| 19|  8|\n",
      "|  4| 24| 31|  6|\n",
      "|  5| 75| 17| 11|\n",
      "|  6| 49| 38| 57|\n",
      "|  7| 56| 31| 90|\n",
      "|  8|100|  3|  3|\n",
      "|  9| 43| 72| 34|\n",
      "| 10| 18| 64| 57|\n",
      "| 11| 63| 75| 80|\n",
      "| 12| 82| 85| 28|\n",
      "| 13| 31|  8| 42|\n",
      "| 14| 20| 80|  3|\n",
      "| 15| 27| 91| 86|\n",
      "| 16| 55| 70| 42|\n",
      "| 17| 69|  3|  5|\n",
      "| 18| 65| 28| 28|\n",
      "| 19| 57|  8| 69|\n",
      "+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_load_read = sparkSession.read.csv(local_dest_path,columns)\n",
    "df_load_read.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 items\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-14 18:53 .sparkStaging\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-13 20:45 example\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-13 21:45 large-data\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-14 18:55 largedata\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-13 21:57 ld-10\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-14 19:00 my-pi\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-13 21:07 mypi\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-13 21:58 sd-5\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-14 19:00 small-data-100\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-14 18:53 smalldata-1000\n"
     ]
    }
   ],
   "source": [
    "!/mnt/code/scripts/my_hdfs.sh dfs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
