{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example which uses on-demand Spark to perform computation and read/write to remote secure HDFS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First get the HDFS Delegation token\n",
    "\n",
    "SPARK communicates with HDFS securely using an HDFS Delegation token. This token is requesting from the HDFS service via identifying the client using the Kerberos ticket (Run `klist` to see the ticket details from your cache).\n",
    "\n",
    "We need to have this ticket accessible by both the driver and the executors. Hence we place it in the DATASET for the project. Ideally we need to have a DATASET per user and only accessible to the user to be contain this ticket for security reasons. \n",
    "\n",
    "**In more ideally this should be supported inside SPARK via a side-car container in both the Worskspace and Spark Nodes (Via Domsed or natively) in transient location such as `/tmp/` folder**.\n",
    "\n",
    "Set the value for this token file in the project environment variable `HADOOP_TOKEN_FILE_LOCATION`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticket cache: FILE:/tmp/krb5cc_12574\n",
      "Default principal: dominospark@KDCDOMINO.COM\n",
      "\n",
      "Valid starting       Expires              Service principal\n",
      "08/18/2021 12:29:44  08/18/2021 22:29:44  krbtgt/KDCDOMINO.COM@KDCDOMINO.COM\n",
      "\trenew until 08/19/2021 12:29:44\n"
     ]
    }
   ],
   "source": [
    "!klist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-18 16:13:49,825 INFO hdfs.DFSClient: Created token for dominospark: HDFS_DELEGATION_TOKEN owner=dominospark@KDCDOMINO.COM, renewer=null, realUser=, issueDate=1629303229812, maxDate=1629908029812, sequenceNumber=125, masterKeyId=6 on 10.0.123.114:8020\n",
      "env: HADOOP_TOKEN_FILE_LOCATION=/mnt/data/ON-DEMAND-SPARK/hdfsdt.token\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#Generate an HDFS Delegation token\n",
    "#%env HADOOP_TOKEN_FILE_LOCATION=/mnt/data/$ON-DEMAND-SPARK/hdfsdt.token\n",
    "#Run the following commands\n",
    "!/mnt/code/scripts/my_hdfs.sh fetchdt --renewer null /mnt/data/$DOMINO_PROJECT_NAME/hdfsdt.token\n",
    "#%env HADOOP_TOKEN_FILE_LOCATION=/mnt/data/$DOMINO_PROJECT_NAME/hdfsdt.token\n",
    "\n",
    "%env HADOOP_TOKEN_FILE_LOCATION=/mnt/data/ON-DEMAND-SPARK/hdfsdt.token\n",
    "\n",
    "hdfs_dt_token_path=os.environ['HADOOP_TOKEN_FILE_LOCATION']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next configure the environment variables `PYSPARK_PYTHON` and `PYSPARK_DRIVER_PYTHON`. This is not necessary if both the workspace and executors use the same path (`/usr/bin/python3`). But if your workspace uses `/opt/conda/bin/python3` your job will fail in the executors. You can set both to  `/opt/conda/bin/python3`/ and see how it fails.\n",
    "\n",
    "Also fetch the HDFS_ENDPOINT from the Project Environment variable `HDFS_ENDPOINT`. In my example the HDFS Namenode location is running on `hdfs://10.0.123.114:8020`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYSPARK_PYTHON=/usr/bin/python3\n",
      "env: PYSPARK_DRIVER_PYTHON=/usr/bin/python3\n"
     ]
    }
   ],
   "source": [
    "%env PYSPARK_PYTHON /usr/bin/python3\n",
    "%env PYSPARK_DRIVER_PYTHON /usr/bin/python3\n",
    "hdfs_endpoint=os.environ['HDFS_ENDPOINT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, DoubleType, IntegerType\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the Spark Session. Note the value of the config `spark.executorEnv.HADOOP_TOKEN_FILE_LOCATION` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On DEMAND SPARK WITH DT\n",
    "\n",
    "\n",
    "sparkSession = SparkSession.builder.appName(\"Calculate Pi using OnDemand Spark\") \\\n",
    ".config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    ".config(\"fs.default.name\", hdfs_endpoint) \\\n",
    ".config(\"spark.driver.extraClassPath\", \"/opt/hadoop/etc/hadoop:/usr/lib/hadoop-lzo/lib/*:/opt/hadoop/share/hadoop/mapreduce/*:/opt/hadoop/share/hadoop/hdfs/*\") \\\n",
    ".config(\"spark.executorEnv.HADOOP_TOKEN_FILE_LOCATION\",hdfs_dt_token_path) \\\n",
    ".getOrCreate()\n",
    "sc=sparkSession.sparkContext\n",
    "#.config(\"spark.executor.extraClassPath\", \"/opt/hadoop/etc/hadoop:/usr/lib/hadoop-lzo/lib/*:/opt/hadoop/share/hadoop/mapreduce/*:/opt/hadoop/share/hadoop/hdfs/*\") \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inside(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|name|value|\n",
      "+----+-----+\n",
      "|  Pi|3.156|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "columns = StructType([ StructField(\"name\", StringType(), True),\n",
    "                      StructField(\"value\", DoubleType(), True)\n",
    "                    ])\n",
    "\n",
    "count = sc.parallelize(range(0, 1000),1) \\\n",
    "             .filter(inside).count()\n",
    "data = [(\"Pi\",4.0 * count/1000)]\n",
    "\n",
    "df = sparkSession.createDataFrame(data=data, schema=columns)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write to secure (Kerberized) HDFS. This uses the hdfs token file we generated in step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use '-rm -r' instead.\n",
      "Deleted /user/dominospark/my-pi\n",
      "+---+-----+\n",
      "|_c0|  _c1|\n",
      "+---+-----+\n",
      "| Pi|3.156|\n",
      "+---+-----+\n",
      "\n",
      "Found 3 items\n",
      "-rw-r--r--   3 dominospark dominospark          0 2021-08-18 16:15 /user/dominospark/my-pi/_SUCCESS\n",
      "-rw-r--r--   3 dominospark dominospark          0 2021-08-18 16:15 /user/dominospark/my-pi/part-00000-ab57bb64-0aed-4b51-aad3-624e442436ca-c000.csv\n",
      "-rw-r--r--   3 dominospark dominospark          9 2021-08-18 16:15 /user/dominospark/my-pi/part-00003-ab57bb64-0aed-4b51-aad3-624e442436ca-c000.csv\n"
     ]
    }
   ],
   "source": [
    "#Let us write to a dataset\n",
    "ds_path = '/user/dominospark/my-pi'\n",
    "!/mnt/code/scripts/my_hdfs.sh dfs -rmr '/user/dominospark/my-pi*'\n",
    "df.write.csv(ds_path)\n",
    "#Read it back\n",
    "sparkSession.read.csv(ds_path).show()\n",
    "!/mnt/code/scripts/my_hdfs.sh dfs -ls /user/dominospark/my-pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we just do more the same. Generate a dataset in HDFS and then retrive it and filter the records based on a simple criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use '-rm -r' instead.\n",
      "Deleted /user/dominospark/small-data-100\n"
     ]
    }
   ],
   "source": [
    "!/mnt/code/scripts/my_hdfs.sh dfs -rmr '/user/dominospark/small-data-100/'\n",
    "hdfs_src_path = '/user/dominospark/largedata/'\n",
    "hdfs_dest_path =  '/user/dominospark/small-data-100/'\n",
    "local_dest_path = 'file:///mnt/data/ON-DEMAND-SPARK/small-data-100'\n",
    "!rm -rf /mnt/data/ON-DEMAND-SPARK/small-data-100\n",
    "filter_criteria = 100\n",
    "'''\n",
    "sparkSession = SparkSession.builder.appName(\"Generate Data\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    "    .config(\"fs.default.name\", hdfs_endpoint) \\\n",
    "    .getOrCreate()\n",
    "'''\n",
    "    \n",
    "columns = StructType([ StructField(\"id\", IntegerType(), True), \\\n",
    "                       StructField(\"v1\", IntegerType(), True),\\\n",
    "                       StructField(\"v2\", IntegerType(), True),\\\n",
    "                       StructField(\"v3\", IntegerType(), True) ])\n",
    "\n",
    "df_load = sparkSession.read.csv(hdfs_src_path,columns)\n",
    "df_load_filtered = df_load.where(df_load.id < filter_criteria)\n",
    "df_load_filtered.write.csv(hdfs_dest_path)\n",
    "df_load_filtered.write.csv(local_dest_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "| id| v1| v2| v3|\n",
      "+---+---+---+---+\n",
      "|  0| 63| 94| 50|\n",
      "|  1| 25| 26| 73|\n",
      "|  2| 84| 84| 84|\n",
      "|  3| 47| 19|  8|\n",
      "|  4| 24| 31|  6|\n",
      "|  5| 75| 17| 11|\n",
      "|  6| 49| 38| 57|\n",
      "|  7| 56| 31| 90|\n",
      "|  8|100|  3|  3|\n",
      "|  9| 43| 72| 34|\n",
      "| 10| 18| 64| 57|\n",
      "| 11| 63| 75| 80|\n",
      "| 12| 82| 85| 28|\n",
      "| 13| 31|  8| 42|\n",
      "| 14| 20| 80|  3|\n",
      "| 15| 27| 91| 86|\n",
      "| 16| 55| 70| 42|\n",
      "| 17| 69|  3|  5|\n",
      "| 18| 65| 28| 28|\n",
      "| 19| 57|  8| 69|\n",
      "+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_load_read = sparkSession.read.csv(hdfs_dest_path,columns)\n",
    "df_load_read.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "| id| v1| v2| v3|\n",
      "+---+---+---+---+\n",
      "|  0| 63| 94| 50|\n",
      "|  1| 25| 26| 73|\n",
      "|  2| 84| 84| 84|\n",
      "|  3| 47| 19|  8|\n",
      "|  4| 24| 31|  6|\n",
      "|  5| 75| 17| 11|\n",
      "|  6| 49| 38| 57|\n",
      "|  7| 56| 31| 90|\n",
      "|  8|100|  3|  3|\n",
      "|  9| 43| 72| 34|\n",
      "| 10| 18| 64| 57|\n",
      "| 11| 63| 75| 80|\n",
      "| 12| 82| 85| 28|\n",
      "| 13| 31|  8| 42|\n",
      "| 14| 20| 80|  3|\n",
      "| 15| 27| 91| 86|\n",
      "| 16| 55| 70| 42|\n",
      "| 17| 69|  3|  5|\n",
      "| 18| 65| 28| 28|\n",
      "| 19| 57|  8| 69|\n",
      "+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_load_read = sparkSession.read.csv(local_dest_path,columns)\n",
    "df_load_read.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 items\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-18 12:59 .sparkStaging\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-18 12:59 example\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-13 21:45 large-data\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-17 15:47 large-data-livy\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-14 18:55 largedata\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-13 21:57 ld-10\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-16 21:13 livy-large-data\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-18 13:05 my-pi\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-18 12:56 mypi\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-13 21:58 sd-5\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-18 13:06 small-data-100\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-14 23:45 smalldata-1000\n"
     ]
    }
   ],
   "source": [
    "!/mnt/code/scripts/my_hdfs.sh dfs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
