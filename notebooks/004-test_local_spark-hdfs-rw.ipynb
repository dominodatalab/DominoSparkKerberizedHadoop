{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running LOCAL spark to read and write to HDFS\n",
    "\n",
    "Local spark runs the drivers and executors in the same JVM. As such it has access to the Kerberos ticket in the cache. \n",
    "\n",
    "Hence we can skip the task of producing a HDFS Delegation token which is produced internally. Local Spark can read write to HDFS without any extra development similar to how we can do the same with EMR Spark (with driver running in client mode) as long as the driver runs inside a workspace with a valid Kerberos ticket (you can examine it by running `kilist` in the terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticket cache: FILE:/tmp/krb5cc_12574\n",
      "Default principal: dominospark@KDCDOMINO.COM\n",
      "\n",
      "Valid starting       Expires              Service principal\n",
      "08/16/2021 12:12:32  08/16/2021 22:12:32  krbtgt/KDCDOMINO.COM@KDCDOMINO.COM\n",
      "\trenew until 08/17/2021 12:12:32\n"
     ]
    }
   ],
   "source": [
    "!klist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HDFS_PATH=/user/dominospark/small_data\n",
      "env: HADOOP_HOME=/usr/lib/hadoop\n",
      "env: HADOOP_CONF_DIR=/etc/hadoop/conf\n",
      "env: HADOOP_YARN_HOME=/usr/lib/hadoop\n",
      "env: HADOOP_MAPRED_HOME=/usr/lib/hadoop\n",
      "env: HADOOP_HDFS_HOME=/usr/lib/hadoop-hdfs\n",
      "env: SPARK_HOME=/usr/lib/spark\n",
      "env: SPARK_CONF_DIR=/etc/spark/conf\n",
      "env: PYSPARK_PYTHON=/opt/conda/bin/python\n",
      "env: PYSPARK_DRIVER_PYTHON=/opt/conda/bin/python\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "%env HDFS_PATH /user/dominospark/small_data\n",
    "%env HADOOP_HOME /usr/lib/hadoop\n",
    "%env HADOOP_CONF_DIR /etc/hadoop/conf\n",
    "%env HADOOP_YARN_HOME /usr/lib/hadoop\n",
    "%env HADOOP_MAPRED_HOME /usr/lib/hadoop\n",
    "%env HADOOP_HDFS_HOME /usr/lib/hadoop-hdfs\n",
    "\n",
    "%env SPARK_HOME /usr/lib/spark\n",
    "%env SPARK_CONF_DIR /etc/spark/conf\n",
    "#%env PYTHONPATH /opt/spark/python/lib/py4j-0.10.7-src.zip\n",
    "%env PYSPARK_PYTHON /opt/conda/bin/python\n",
    "%env PYSPARK_DRIVER_PYTHON /opt/conda/bin/python\n",
    "hdfs_endpoint=os.environ['HDFS_ENDPOINT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, DoubleType, IntegerType\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession = SparkSession.builder.appName(\"example-pyspark-read-and-write\") \\\n",
    ".master('local[*]') \\\n",
    ".config(\"fs.default.name\", hdfs_endpoint) \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=sparkSession.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inside(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|name|value|\n",
      "+----+-----+\n",
      "|  Pi|3.104|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "columns = StructType([ StructField(\"name\", StringType(), True),\n",
    "                      StructField(\"value\", DoubleType(), True)\n",
    "                    ])\n",
    "\n",
    "count = sc.parallelize(range(0, 1000),1) \\\n",
    "             .filter(inside).count()\n",
    "data = [(\"Pi\",4.0 * count/1000)]\n",
    "\n",
    "df = sparkSession.createDataFrame(data=data, schema=columns)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use '-rm -r' instead.\n",
      "Deleted /user/dominospark/my-pi\n",
      "+---+-----+\n",
      "|_c0|  _c1|\n",
      "+---+-----+\n",
      "| Pi|3.104|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Let us write to a dataset\n",
    "ds_path = '/user/dominospark/my-pi'\n",
    "!/mnt/code/scripts/my_hdfs.sh dfs -rmr '/user/dominospark/my-pi*'\n",
    "df.write.csv(ds_path)\n",
    "#Read it back\n",
    "sparkSession.read.csv(ds_path).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use '-rm -r' instead.\n",
      "Deleted /user/dominospark/small-data-100\n"
     ]
    }
   ],
   "source": [
    "!/mnt/code/scripts/my_hdfs.sh dfs -rmr '/user/dominospark/small-data-100/'\n",
    "hdfs_src_path = '/user/dominospark/largedata/'\n",
    "hdfs_dest_path =  '/user/dominospark/small-data-100/'\n",
    "local_dest_path = 'file:///mnt/data/ON-DEMAND-SPARK/small-data-100'\n",
    "!rm -rf /mnt/data/ON-DEMAND-SPARK/small-data-100\n",
    "filter_criteria = 100\n",
    "sparkSession = SparkSession.builder.appName(\"Generate Data\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    "    .config(\"fs.default.name\", hdfs_endpoint) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "    \n",
    "columns = StructType([ StructField(\"id\", IntegerType(), True), \\\n",
    "                       StructField(\"v1\", IntegerType(), True),\\\n",
    "                       StructField(\"v2\", IntegerType(), True),\\\n",
    "                       StructField(\"v3\", IntegerType(), True) ])\n",
    "\n",
    "df_load = sparkSession.read.csv(hdfs_src_path,columns)\n",
    "df_load_filtered = df_load.where(df_load.id < filter_criteria)\n",
    "df_load_filtered.write.csv(hdfs_dest_path)\n",
    "df_load_filtered.write.csv(local_dest_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "| id| v1| v2| v3|\n",
      "+---+---+---+---+\n",
      "|  0| 63| 94| 50|\n",
      "|  1| 25| 26| 73|\n",
      "|  2| 84| 84| 84|\n",
      "|  3| 47| 19|  8|\n",
      "|  4| 24| 31|  6|\n",
      "|  5| 75| 17| 11|\n",
      "|  6| 49| 38| 57|\n",
      "|  7| 56| 31| 90|\n",
      "|  8|100|  3|  3|\n",
      "|  9| 43| 72| 34|\n",
      "| 10| 18| 64| 57|\n",
      "| 11| 63| 75| 80|\n",
      "| 12| 82| 85| 28|\n",
      "| 13| 31|  8| 42|\n",
      "| 14| 20| 80|  3|\n",
      "| 15| 27| 91| 86|\n",
      "| 16| 55| 70| 42|\n",
      "| 17| 69|  3|  5|\n",
      "| 18| 65| 28| 28|\n",
      "| 19| 57|  8| 69|\n",
      "+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_load_read = sparkSession.read.csv(hdfs_dest_path,columns)\n",
    "df_load_read.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "| id| v1| v2| v3|\n",
      "+---+---+---+---+\n",
      "|  0| 63| 94| 50|\n",
      "|  1| 25| 26| 73|\n",
      "|  2| 84| 84| 84|\n",
      "|  3| 47| 19|  8|\n",
      "|  4| 24| 31|  6|\n",
      "|  5| 75| 17| 11|\n",
      "|  6| 49| 38| 57|\n",
      "|  7| 56| 31| 90|\n",
      "|  8|100|  3|  3|\n",
      "|  9| 43| 72| 34|\n",
      "| 10| 18| 64| 57|\n",
      "| 11| 63| 75| 80|\n",
      "| 12| 82| 85| 28|\n",
      "| 13| 31|  8| 42|\n",
      "| 14| 20| 80|  3|\n",
      "| 15| 27| 91| 86|\n",
      "| 16| 55| 70| 42|\n",
      "| 17| 69|  3|  5|\n",
      "| 18| 65| 28| 28|\n",
      "| 19| 57|  8| 69|\n",
      "+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_load_read = sparkSession.read.csv(local_dest_path,columns)\n",
    "df_load_read.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 items\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-14 18:53 .sparkStaging\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-13 20:45 example\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-13 21:45 large-data\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-14 18:55 largedata\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-13 21:57 ld-10\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-14 19:27 my-pi\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-13 21:07 mypi\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-13 21:58 sd-5\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-14 19:27 small-data-100\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-14 19:22 smalldata-10\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-14 18:53 smalldata-1000\n"
     ]
    }
   ],
   "source": [
    "!/mnt/code/scripts/my_hdfs.sh dfs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/spark'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
