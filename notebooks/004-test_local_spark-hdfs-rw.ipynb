{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running LOCAL spark to read and write to HDFS\n",
    "\n",
    "Local spark runs the drivers and executors in the same JVM. As such it has access to the Kerberos ticket in the cache. \n",
    "\n",
    "Hence we can skip the task of producing a HDFS Delegation token which is produced internally. Local Spark can read write to HDFS without any extra development similar to how we can do the same with EMR Spark (with driver running in client mode) as long as the driver runs inside a workspace with a valid Kerberos ticket (you can examine it by running `kilist` in the terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!klist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "%env HDFS_PATH /user/dominospark/small_data\n",
    "%env HADOOP_HOME /usr/lib/hadoop\n",
    "%env HADOOP_CONF_DIR /etc/hadoop/conf\n",
    "%env HADOOP_YARN_HOME /usr/lib/hadoop\n",
    "%env HADOOP_MAPRED_HOME /usr/lib/hadoop\n",
    "%env HADOOP_HDFS_HOME /usr/lib/hadoop-hdfs\n",
    "\n",
    "%env SPARK_HOME /usr/lib/spark\n",
    "%env SPARK_CONF_DIR /etc/spark/conf\n",
    "#%env PYTHONPATH /opt/spark/python/lib/py4j-0.10.7-src.zip\n",
    "%env PYSPARK_PYTHON /opt/conda/bin/python\n",
    "%env PYSPARK_DRIVER_PYTHON /opt/conda/bin/python\n",
    "hdfs_endpoint=os.environ['HDFS_ENDPOINT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, DoubleType, IntegerType\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession = SparkSession.builder.appName(\"example-pyspark-read-and-write\") \\\n",
    ".master('local[*]') \\\n",
    ".config(\"fs.default.name\", hdfs_endpoint) \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=sparkSession.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inside(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "columns = StructType([ StructField(\"name\", StringType(), True),\n",
    "                      StructField(\"value\", DoubleType(), True)\n",
    "                    ])\n",
    "\n",
    "count = sc.parallelize(range(0, 1000),1) \\\n",
    "             .filter(inside).count()\n",
    "data = [(\"Pi\",4.0 * count/1000)]\n",
    "\n",
    "df = sparkSession.createDataFrame(data=data, schema=columns)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us write to a dataset\n",
    "ds_path = '/user/dominospark/my-pi'\n",
    "!/mnt/code/scripts/my_hdfs.sh dfs -rmr '/user/dominospark/my-pi*'\n",
    "df.write.csv(ds_path)\n",
    "#Read it back\n",
    "sparkSession.read.csv(ds_path).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/mnt/code/scripts/my_hdfs.sh dfs -rmr '/user/dominospark/small-data-100/'\n",
    "hdfs_src_path = '/user/dominospark/largedata/'\n",
    "hdfs_dest_path =  '/user/dominospark/small-data-100/'\n",
    "local_dest_path = 'file:///mnt/data/ON-DEMAND-SPARK/small-data-100'\n",
    "!rm -rf /mnt/data/ON-DEMAND-SPARK/small-data-100\n",
    "filter_criteria = 100\n",
    "sparkSession = SparkSession.builder.appName(\"Generate Data\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    "    .config(\"fs.default.name\", hdfs_endpoint) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "    \n",
    "columns = StructType([ StructField(\"id\", IntegerType(), True), \\\n",
    "                       StructField(\"v1\", IntegerType(), True),\\\n",
    "                       StructField(\"v2\", IntegerType(), True),\\\n",
    "                       StructField(\"v3\", IntegerType(), True) ])\n",
    "\n",
    "df_load = sparkSession.read.csv(hdfs_src_path,columns)\n",
    "df_load_filtered = df_load.where(df_load.id < filter_criteria)\n",
    "df_load_filtered.write.csv(hdfs_dest_path)\n",
    "df_load_filtered.write.csv(local_dest_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_load_read = sparkSession.read.csv(hdfs_dest_path,columns)\n",
    "df_load_read.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_load_read = sparkSession.read.csv(local_dest_path,columns)\n",
    "df_load_read.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/mnt/code/scripts/my_hdfs.sh dfs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
