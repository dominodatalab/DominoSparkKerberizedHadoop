{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use '-rm -r' instead.\n",
      "rmr: `/user/dominospark/smalldata-1000/': No such file or directory\n",
      "log4j:ERROR setFile(null,true) call failed.\n",
      "java.io.FileNotFoundException: /var/log/spark/user/ubuntu/stderr (No such file or directory)\n",
      "\tat java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\n",
      "\tat java.io.FileOutputStream.<init>(FileOutputStream.java:213)\n",
      "\tat java.io.FileOutputStream.<init>(FileOutputStream.java:133)\n",
      "\tat org.apache.log4j.FileAppender.setFile(FileAppender.java:294)\n",
      "\tat org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165)\n",
      "\tat org.apache.log4j.DailyRollingFileAppender.activateOptions(DailyRollingFileAppender.java:223)\n",
      "\tat org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307)\n",
      "\tat org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:172)\n",
      "\tat org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:104)\n",
      "\tat org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:842)\n",
      "\tat org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768)\n",
      "\tat org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:672)\n",
      "\tat org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:516)\n",
      "\tat org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:580)\n",
      "\tat org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)\n",
      "\tat org.apache.log4j.LogManager.<clinit>(LogManager.java:127)\n",
      "\tat org.slf4j.impl.Log4jLoggerFactory.<init>(Log4jLoggerFactory.java:66)\n",
      "\tat org.slf4j.impl.StaticLoggerBinder.<init>(StaticLoggerBinder.java:72)\n",
      "\tat org.slf4j.impl.StaticLoggerBinder.<clinit>(StaticLoggerBinder.java:45)\n",
      "\tat org.apache.spark.internal.Logging$.org$apache$spark$internal$Logging$$isLog4j12(Logging.scala:222)\n",
      "\tat org.apache.spark.internal.Logging.initializeLogging(Logging.scala:127)\n",
      "\tat org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:111)\n",
      "\tat org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:105)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.initializeLogIfNecessary(SparkSubmit.scala:75)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:83)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1038)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1047)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "log4j:ERROR Either File or DatePattern options are not set for appender [DRFA-stderr].\n",
      "log4j:ERROR setFile(null,true) call failed.\n",
      "java.io.FileNotFoundException: /var/log/spark/user/ubuntu/stdout (No such file or directory)\n",
      "\tat java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\n",
      "\tat java.io.FileOutputStream.<init>(FileOutputStream.java:213)\n",
      "\tat java.io.FileOutputStream.<init>(FileOutputStream.java:133)\n",
      "\tat org.apache.log4j.FileAppender.setFile(FileAppender.java:294)\n",
      "\tat org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165)\n",
      "\tat org.apache.log4j.DailyRollingFileAppender.activateOptions(DailyRollingFileAppender.java:223)\n",
      "\tat org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307)\n",
      "\tat org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:172)\n",
      "\tat org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:104)\n",
      "\tat org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:842)\n",
      "\tat org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768)\n",
      "\tat org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:672)\n",
      "\tat org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:516)\n",
      "\tat org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:580)\n",
      "\tat org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)\n",
      "\tat org.apache.log4j.LogManager.<clinit>(LogManager.java:127)\n",
      "\tat org.slf4j.impl.Log4jLoggerFactory.<init>(Log4jLoggerFactory.java:66)\n",
      "\tat org.slf4j.impl.StaticLoggerBinder.<init>(StaticLoggerBinder.java:72)\n",
      "\tat org.slf4j.impl.StaticLoggerBinder.<clinit>(StaticLoggerBinder.java:45)\n",
      "\tat org.apache.spark.internal.Logging$.org$apache$spark$internal$Logging$$isLog4j12(Logging.scala:222)\n",
      "\tat org.apache.spark.internal.Logging.initializeLogging(Logging.scala:127)\n",
      "\tat org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:111)\n",
      "\tat org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:105)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.initializeLogIfNecessary(SparkSubmit.scala:75)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:83)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1038)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1047)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "log4j:ERROR Either File or DatePattern options are not set for appender [DRFA-stdout].\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "21/08/13 17:50:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "21/08/13 17:50:16 INFO RMProxy: Connecting to ResourceManager at ip-10-0-1-76.us-west-2.compute.internal/10.0.1.76:8032\n",
      "21/08/13 17:50:17 INFO Client: Requesting a new application from cluster with 2 NodeManagers\n",
      "21/08/13 17:50:17 INFO Configuration: resource-types.xml not found\n",
      "21/08/13 17:50:17 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "21/08/13 17:50:17 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)\n",
      "21/08/13 17:50:17 INFO Client: Will allocate AM container, with 2432 MB memory including 384 MB overhead\n",
      "21/08/13 17:50:17 INFO Client: Setting up container launch context for our AM\n",
      "21/08/13 17:50:17 INFO Client: Setting up the launch environment for our AM container\n",
      "21/08/13 17:50:17 INFO Client: Preparing resources for our AM container\n",
      "21/08/13 17:50:17 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "21/08/13 17:50:20 INFO Client: Uploading resource file:/tmp/spark-576d8329-f8a7-4a09-a174-53c07cd619b7/__spark_libs__4738225226385213333.zip -> hdfs://ip-10-0-1-76.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628691104481_0088/__spark_libs__4738225226385213333.zip\n",
      "21/08/13 17:50:22 INFO Client: Uploading resource file:/etc/spark/conf/hive-site.xml -> hdfs://ip-10-0-1-76.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628691104481_0088/hive-site.xml\n",
      "21/08/13 17:50:22 INFO Client: Uploading resource file:/mnt/code/python/filter_data_in_hdfs.py -> hdfs://ip-10-0-1-76.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628691104481_0088/filter_data_in_hdfs.py\n",
      "21/08/13 17:50:22 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-10-0-1-76.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628691104481_0088/pyspark.zip\n",
      "21/08/13 17:50:22 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9-src.zip -> hdfs://ip-10-0-1-76.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628691104481_0088/py4j-0.10.9-src.zip\n",
      "21/08/13 17:50:22 INFO Client: Uploading resource file:/tmp/spark-576d8329-f8a7-4a09-a174-53c07cd619b7/__spark_conf__2231920829885977854.zip -> hdfs://ip-10-0-1-76.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628691104481_0088/__spark_conf__.zip\n",
      "21/08/13 17:50:22 INFO SecurityManager: Changing view acls to: ubuntu,dominospark\n",
      "21/08/13 17:50:22 INFO SecurityManager: Changing modify acls to: ubuntu,dominospark\n",
      "21/08/13 17:50:22 INFO SecurityManager: Changing view acls groups to: \n",
      "21/08/13 17:50:22 INFO SecurityManager: Changing modify acls groups to: \n",
      "21/08/13 17:50:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ubuntu, dominospark); groups with view permissions: Set(); users  with modify permissions: Set(ubuntu, dominospark); groups with modify permissions: Set()\n",
      "21/08/13 17:50:22 INFO HadoopDelegationTokenManager: Attempting to load user's ticket cache.\n",
      "21/08/13 17:50:22 INFO HadoopFSDelegationTokenProvider: getting token for: DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_438121000_1, ugi=dominospark@KDCDOMINO.COM (auth:KERBEROS)]] with renewer yarn/ip-10-0-1-76.us-west-2.compute.internal@KDCDOMINO.COM\n",
      "21/08/13 17:50:22 INFO DFSClient: Created token for dominospark: HDFS_DELEGATION_TOKEN owner=dominospark@KDCDOMINO.COM, renewer=yarn, realUser=, issueDate=1628877022825, maxDate=1629481822825, sequenceNumber=174, masterKeyId=4 on 10.0.1.76:8020\n",
      "21/08/13 17:50:23 INFO KMSClientProvider: New token created: (Kind: kms-dt, Service: kms://http@ip-10-0-1-76.us-west-2.compute.internal:9600/kms, Ident: (kms-dt owner=dominospark, renewer=yarn, realUser=, issueDate=1628877022939, maxDate=1629481822939, sequenceNumber=174, masterKeyId=4))\n",
      "21/08/13 17:50:23 INFO HadoopFSDelegationTokenProvider: getting token for: DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_438121000_1, ugi=dominospark@KDCDOMINO.COM (auth:KERBEROS)]] with renewer dominospark@KDCDOMINO.COM\n",
      "21/08/13 17:50:23 INFO DFSClient: Created token for dominospark: HDFS_DELEGATION_TOKEN owner=dominospark@KDCDOMINO.COM, renewer=dominospark, realUser=, issueDate=1628877023351, maxDate=1629481823351, sequenceNumber=175, masterKeyId=4 on 10.0.1.76:8020\n",
      "21/08/13 17:50:23 INFO KMSClientProvider: New token created: (Kind: kms-dt, Service: kms://http@ip-10-0-1-76.us-west-2.compute.internal:9600/kms, Ident: (kms-dt owner=dominospark, renewer=dominospark, realUser=, issueDate=1628877023424, maxDate=1629481823424, sequenceNumber=175, masterKeyId=4))\n",
      "21/08/13 17:50:23 INFO HadoopFSDelegationTokenProvider: Renewal interval is 86400019 for token kms-dt\n",
      "21/08/13 17:50:23 INFO HadoopFSDelegationTokenProvider: Renewal interval is 86400097 for token HDFS_DELEGATION_TOKEN\n",
      "21/08/13 17:50:25 INFO HiveConf: Found configuration file file:/etc/spark/conf/hive-site.xml\n",
      "21/08/13 17:50:25 WARN HiveConf: HiveConf of name hive.server2.thrift.url does not exist\n",
      "21/08/13 17:50:25 WARN HiveConf: HiveConf of name hive.server2.thrift.url does not exist\n",
      "21/08/13 17:50:26 INFO metastore: Trying to connect to metastore with URI thrift://ip-10-0-1-76.us-west-2.compute.internal:9083\n",
      "21/08/13 17:50:26 INFO metastore: Opened a connection to metastore, current connections: 1\n",
      "21/08/13 17:50:26 INFO metastore: Connected to metastore.\n",
      "21/08/13 17:50:26 INFO metastore: Closed a connection to metastore, current connections: 0\n",
      "21/08/13 17:50:26 INFO Client: Submitting application application_1628691104481_0088 to ResourceManager\n",
      "21/08/13 17:50:26 INFO YarnClientImpl: Submitted application application_1628691104481_0088\n",
      "21/08/13 17:50:27 INFO Client: Application report for application_1628691104481_0088 (state: ACCEPTED)\n",
      "21/08/13 17:50:27 INFO Client: \n",
      "\t client token: Token { kind: YARN_CLIENT_TOKEN, service:  }\n",
      "\t diagnostics: AM container is launched, waiting for AM container to Register with RM\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1628877026320\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-10-0-1-76.us-west-2.compute.internal:20888/proxy/application_1628691104481_0088/\n",
      "\t user: dominospark\n",
      "21/08/13 17:50:28 INFO Client: Application report for application_1628691104481_0088 (state: ACCEPTED)\n",
      "21/08/13 17:50:29 INFO Client: Application report for application_1628691104481_0088 (state: ACCEPTED)\n",
      "21/08/13 17:50:30 INFO Client: Application report for application_1628691104481_0088 (state: ACCEPTED)\n",
      "21/08/13 17:50:31 INFO Client: Application report for application_1628691104481_0088 (state: ACCEPTED)\n",
      "21/08/13 17:50:32 INFO Client: Application report for application_1628691104481_0088 (state: ACCEPTED)\n",
      "21/08/13 17:50:33 INFO Client: Application report for application_1628691104481_0088 (state: ACCEPTED)\n",
      "21/08/13 17:50:34 INFO Client: Application report for application_1628691104481_0088 (state: RUNNING)\n",
      "21/08/13 17:50:34 INFO Client: \n",
      "\t client token: Token { kind: YARN_CLIENT_TOKEN, service:  }\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: ip-10-0-1-80.us-west-2.compute.internal\n",
      "\t ApplicationMaster RPC port: 32885\n",
      "\t queue: default\n",
      "\t start time: 1628877026320\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-10-0-1-76.us-west-2.compute.internal:20888/proxy/application_1628691104481_0088/\n",
      "\t user: dominospark\n",
      "21/08/13 17:50:35 INFO Client: Application report for application_1628691104481_0088 (state: RUNNING)\n",
      "21/08/13 17:50:36 INFO Client: Application report for application_1628691104481_0088 (state: RUNNING)\n",
      "21/08/13 17:50:37 INFO Client: Application report for application_1628691104481_0088 (state: RUNNING)\n",
      "21/08/13 17:50:38 INFO Client: Application report for application_1628691104481_0088 (state: RUNNING)\n",
      "21/08/13 17:50:39 INFO Client: Application report for application_1628691104481_0088 (state: RUNNING)\n",
      "21/08/13 17:50:40 INFO Client: Application report for application_1628691104481_0088 (state: RUNNING)\n",
      "21/08/13 17:50:41 INFO Client: Application report for application_1628691104481_0088 (state: RUNNING)\n",
      "21/08/13 17:50:42 INFO Client: Application report for application_1628691104481_0088 (state: RUNNING)\n",
      "21/08/13 17:50:43 INFO Client: Application report for application_1628691104481_0088 (state: RUNNING)\n",
      "21/08/13 17:50:44 INFO Client: Application report for application_1628691104481_0088 (state: RUNNING)\n",
      "21/08/13 17:50:45 INFO Client: Application report for application_1628691104481_0088 (state: FINISHED)\n",
      "21/08/13 17:50:45 INFO Client: \n",
      "\t client token: Token { kind: YARN_CLIENT_TOKEN, service:  }\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: ip-10-0-1-80.us-west-2.compute.internal\n",
      "\t ApplicationMaster RPC port: 32885\n",
      "\t queue: default\n",
      "\t start time: 1628877026320\n",
      "\t final status: SUCCEEDED\n",
      "\t tracking URL: http://ip-10-0-1-76.us-west-2.compute.internal:20888/proxy/application_1628691104481_0088/\n",
      "\t user: dominospark\n",
      "21/08/13 17:50:45 INFO Client: Deleted staging directory hdfs://ip-10-0-1-76.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628691104481_0088\n",
      "21/08/13 17:50:45 INFO ShutdownHookManager: Shutdown hook called\n",
      "21/08/13 17:50:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-576d8329-f8a7-4a09-a174-53c07cd619b7\n",
      "21/08/13 17:50:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-e08c613c-6103-4b8d-940a-e1498da70e42\n"
     ]
    }
   ],
   "source": [
    "!/mnt/code/scripts/filter-data-emr.sh \\\n",
    "1000 hdfs://10.0.1.76:8020 /user/dominospark/largedata/ /user/dominospark/smalldata-1000/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /mnt/data/ON_DEMAND_SPARK/smalldata-1000\n",
    "!/mnt/code/scripts/my_hdfs.sh dfs -copyToLocal /user/dominospark/smalldata-1000/* /mnt/data/ON_DEMAND_SPARK/smalldata-1000/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYSPARK_PYTHON=/usr/bin/python3\n",
      "env: PYSPARK_DRIVER_PYTHON=/usr/bin/python3\n"
     ]
    }
   ],
   "source": [
    "%env PYSPARK_PYTHON /usr/bin/python3\n",
    "%env PYSPARK_DRIVER_PYTHON /usr/bin/python3\n",
    "#%env SPARK_DIST_CLASSPATH  /etc/hadoop/conf:/opt/hadoop/share/hadoop/common/lib/*:/opt/hadoop/share/hadoop/common/*:/usr/lib/hadoop-lzo/lib/hadoop-lzo-0.4.19.jar:/usr/lib/hadoop-lzo/lib/hadoop-lzo.jar:/usr/lib/hadoop-lzo/lib/native:/usr/share/aws/aws-java-sdk/aws-java-sdk-bundle-1.11.977.jar:/usr/share/aws/aws-java-sdk/LICENSE.txt:/usr/share/aws/aws-java-sdk/NOTICE.txt:/usr/share/aws/aws-java-sdk/README.md:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/annotations-16.0.2.jar:/usr/share/aws/emr/emrfs/lib/aopalliance-1.0.jar:/usr/share/aws/emr/emrfs/lib/aws-glue-sdk-1.12.0.jar:/usr/share/aws/emr/emrfs/lib/bcprov-ext-jdk15on-1.66.jar:/usr/share/aws/emr/emrfs/lib/emrfs-hadoop-assembly-2.46.0.jar:/usr/share/aws/emr/emrfs/lib/fluent-hc-4.5.6.jar:/usr/share/aws/emr/emrfs/lib/ion-java-1.0.2.jar:/usr/share/aws/emr/emrfs/lib/javax.inject-1.jar:/usr/share/aws/emr/emrfs/lib/jcl-over-slf4j-1.7.21.jar:/usr/share/aws/emr/emrfs/lib/jmespath-java-1.11.852.jar:/usr/share/aws/emr/emrfs/lib/lombok-1.18.4.jar:/usr/share/aws/emr/emrfs/lib/mockito-core-1.10.19.jar:/usr/share/aws/emr/emrfs/lib/objenesis-2.1.jar:/usr/share/aws/emr/emrfs/lib/secret-agent-interface-1.4.0.jar:/usr/share/aws/emr/emrfs/lib/slf4j-api-1.7.21.jar:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/ddb/lib/emr-ddb-hadoop.jar:/usr/share/aws/emr/goodies/lib/emr-hadoop-goodies.jar:/usr/share/aws/emr/kinesis/lib/emr-kinesis-hadoop.jar:/usr/share/aws/emr/cloudwatch-sink/lib/cloudwatch-sink-2.1.0.jar:/usr/share/aws/emr/cloudwatch-sink/lib/cloudwatch-sink.jar:/opt/hadoop/share/hadoop/tools/lib/*:/usr/lib/hadoop-lzo/lib/*\n",
    "#%env JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, DoubleType, IntegerType\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession = SparkSession.builder.appName(\"ML On Smaller Datasets Copied from HDFS\") \\\n",
    ".config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    ".config(\"spark.driver.extraClassPath\", \"/opt/hadoop/etc/hadoop:/usr/lib/hadoop-lzo/lib/*:/opt/hadoop/share/hadoop/mapreduce/*:/opt/hadoop/share/hadoop/hdfs/*\") \\\n",
    ".getOrCreate()\n",
    "sc=sparkSession.sparkContext\n",
    "#.config(\"spark.executor.extraClassPath\", \"/opt/hadoop/etc/hadoop:/usr/lib/hadoop-lzo/lib/*:/opt/hadoop/share/hadoop/mapreduce/*:/opt/hadoop/share/hadoop/hdfs/*\") \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+---+\n",
      "|   id| v1| v2| v3|\n",
      "+-----+---+---+---+\n",
      "|50176| 57| 34| 69|\n",
      "|50177|  3| 35| 97|\n",
      "|50178| 84| 22| 56|\n",
      "|50179| 12|  5| 69|\n",
      "|50180| 90| 28| 19|\n",
      "|50181| 15|  2| 54|\n",
      "|50182| 98| 24| 38|\n",
      "|50183| 44| 10| 10|\n",
      "|50184| 65|  5| 34|\n",
      "|50185| 52| 54| 91|\n",
      "|50186| 54| 87| 75|\n",
      "|50187| 56| 44| 34|\n",
      "|50188| 32| 81|100|\n",
      "|50189| 22|  1|  2|\n",
      "|50190| 82| 20| 81|\n",
      "|50191| 74| 36| 60|\n",
      "|50192| 97| 94| 70|\n",
      "|50193| 60| 36| 42|\n",
      "|50194| 64| 13|  9|\n",
      "|50195| 78|  3| 14|\n",
      "+-----+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = StructType([ StructField(\"id\", IntegerType(), True),\n",
    "                       StructField(\"v1\", IntegerType(), True),\n",
    "                       StructField(\"v2\", IntegerType(), True),\n",
    "                       StructField(\"v3\", IntegerType(), True) ])\n",
    "df_load = sparkSession.read.csv('file:///mnt/data/ON_DEMAND_SPARK/smalldata-1000',schema=columns)\n",
    "df_load.show()\n",
    "df_load.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|name|value|\n",
      "+----+-----+\n",
      "|  Pi|3.128|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "columns = StructType([ StructField(\"name\", StringType(), True),\n",
    "                      StructField(\"value\", DoubleType(), True)\n",
    "                    ])\n",
    "\n",
    "count = sc.parallelize(range(0, 1000),1) \\\n",
    "             .filter(inside).count()\n",
    "data = [(\"Pi\",4.0 * count/1000)]\n",
    "\n",
    "df = sparkSession.createDataFrame(data=data, schema=columns)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|_c0|  _c1|\n",
      "+---+-----+\n",
      "| Pi|3.128|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Let us write to a dataset\n",
    "ds_path = 'file:///mnt/data/ON_DEMAND_SPARK/pi'\n",
    "df.write.csv(ds_path)\n",
    "#Read it back\n",
    "sparkSession.read.csv(ds_path).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-12 22:05 .sparkStaging\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-12 22:06 example\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-12 21:36 mypi\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-11 15:36 test\n"
     ]
    }
   ],
   "source": [
    "!/mnt/code/scripts/my_hdfs.sh dfs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
