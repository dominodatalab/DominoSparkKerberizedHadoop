{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use '-rm -r' instead.\n",
      "rmr: `/user/dominospark/smalldata-1000/': No such file or directory\n",
      "rmr: DEPRECATED: Please use '-rm -r' instead.\n",
      "rmr: `/user/dominospark/smalldata-1000/': No such file or directory\n",
      "log4j:ERROR setFile(null,true) call failed.\n",
      "java.io.FileNotFoundException: /var/log/spark/user/ubuntu/stderr (No such file or directory)\n",
      "\tat java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\n",
      "\tat java.io.FileOutputStream.<init>(FileOutputStream.java:213)\n",
      "\tat java.io.FileOutputStream.<init>(FileOutputStream.java:133)\n",
      "\tat org.apache.log4j.FileAppender.setFile(FileAppender.java:294)\n",
      "\tat org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165)\n",
      "\tat org.apache.log4j.DailyRollingFileAppender.activateOptions(DailyRollingFileAppender.java:223)\n",
      "\tat org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307)\n",
      "\tat org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:172)\n",
      "\tat org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:104)\n",
      "\tat org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:842)\n",
      "\tat org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768)\n",
      "\tat org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:672)\n",
      "\tat org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:516)\n",
      "\tat org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:580)\n",
      "\tat org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)\n",
      "\tat org.apache.log4j.LogManager.<clinit>(LogManager.java:127)\n",
      "\tat org.slf4j.impl.Log4jLoggerFactory.<init>(Log4jLoggerFactory.java:66)\n",
      "\tat org.slf4j.impl.StaticLoggerBinder.<init>(StaticLoggerBinder.java:72)\n",
      "\tat org.slf4j.impl.StaticLoggerBinder.<clinit>(StaticLoggerBinder.java:45)\n",
      "\tat org.apache.spark.internal.Logging$.org$apache$spark$internal$Logging$$isLog4j12(Logging.scala:222)\n",
      "\tat org.apache.spark.internal.Logging.initializeLogging(Logging.scala:127)\n",
      "\tat org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:111)\n",
      "\tat org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:105)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.initializeLogIfNecessary(SparkSubmit.scala:75)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:83)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1038)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1047)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "log4j:ERROR Either File or DatePattern options are not set for appender [DRFA-stderr].\n",
      "log4j:ERROR setFile(null,true) call failed.\n",
      "java.io.FileNotFoundException: /var/log/spark/user/ubuntu/stdout (No such file or directory)\n",
      "\tat java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\n",
      "\tat java.io.FileOutputStream.<init>(FileOutputStream.java:213)\n",
      "\tat java.io.FileOutputStream.<init>(FileOutputStream.java:133)\n",
      "\tat org.apache.log4j.FileAppender.setFile(FileAppender.java:294)\n",
      "\tat org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165)\n",
      "\tat org.apache.log4j.DailyRollingFileAppender.activateOptions(DailyRollingFileAppender.java:223)\n",
      "\tat org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307)\n",
      "\tat org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:172)\n",
      "\tat org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:104)\n",
      "\tat org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:842)\n",
      "\tat org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768)\n",
      "\tat org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:672)\n",
      "\tat org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:516)\n",
      "\tat org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:580)\n",
      "\tat org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)\n",
      "\tat org.apache.log4j.LogManager.<clinit>(LogManager.java:127)\n",
      "\tat org.slf4j.impl.Log4jLoggerFactory.<init>(Log4jLoggerFactory.java:66)\n",
      "\tat org.slf4j.impl.StaticLoggerBinder.<init>(StaticLoggerBinder.java:72)\n",
      "\tat org.slf4j.impl.StaticLoggerBinder.<clinit>(StaticLoggerBinder.java:45)\n",
      "\tat org.apache.spark.internal.Logging$.org$apache$spark$internal$Logging$$isLog4j12(Logging.scala:222)\n",
      "\tat org.apache.spark.internal.Logging.initializeLogging(Logging.scala:127)\n",
      "\tat org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:111)\n",
      "\tat org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:105)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.initializeLogIfNecessary(SparkSubmit.scala:75)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:83)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1038)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1047)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "log4j:ERROR Either File or DatePattern options are not set for appender [DRFA-stdout].\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "21/08/13 21:51:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "21/08/13 21:51:49 INFO RMProxy: Connecting to ResourceManager at ip-10-0-123-114.us-west-2.compute.internal/10.0.123.114:8032\n",
      "21/08/13 21:51:49 INFO Client: Requesting a new application from cluster with 2 NodeManagers\n",
      "21/08/13 21:51:50 INFO Configuration: resource-types.xml not found\n",
      "21/08/13 21:51:50 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "21/08/13 21:51:50 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)\n",
      "21/08/13 21:51:50 INFO Client: Will allocate AM container, with 2432 MB memory including 384 MB overhead\n",
      "21/08/13 21:51:50 INFO Client: Setting up container launch context for our AM\n",
      "21/08/13 21:51:50 INFO Client: Setting up the launch environment for our AM container\n",
      "21/08/13 21:51:50 INFO Client: Preparing resources for our AM container\n",
      "21/08/13 21:51:50 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "21/08/13 21:51:52 INFO Client: Uploading resource file:/tmp/spark-f5cc1037-c5ce-4c34-99cb-9495047d71a4/__spark_libs__6840414941176588461.zip -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0017/__spark_libs__6840414941176588461.zip\n",
      "21/08/13 21:51:54 INFO Client: Uploading resource file:/etc/spark/conf/hive-site.xml -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0017/hive-site.xml\n",
      "21/08/13 21:51:54 INFO Client: Uploading resource file:/mnt/code/python/filter_data_in_hdfs.py -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0017/filter_data_in_hdfs.py\n",
      "21/08/13 21:51:54 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0017/pyspark.zip\n",
      "21/08/13 21:51:54 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9-src.zip -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0017/py4j-0.10.9-src.zip\n",
      "21/08/13 21:51:54 INFO Client: Uploading resource file:/tmp/spark-f5cc1037-c5ce-4c34-99cb-9495047d71a4/__spark_conf__7670395313443809789.zip -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0017/__spark_conf__.zip\n",
      "21/08/13 21:51:54 INFO SecurityManager: Changing view acls to: ubuntu,dominospark\n",
      "21/08/13 21:51:54 INFO SecurityManager: Changing modify acls to: ubuntu,dominospark\n",
      "21/08/13 21:51:54 INFO SecurityManager: Changing view acls groups to: \n",
      "21/08/13 21:51:54 INFO SecurityManager: Changing modify acls groups to: \n",
      "21/08/13 21:51:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ubuntu, dominospark); groups with view permissions: Set(); users  with modify permissions: Set(ubuntu, dominospark); groups with modify permissions: Set()\n",
      "21/08/13 21:51:54 INFO HadoopDelegationTokenManager: Attempting to load user's ticket cache.\n",
      "21/08/13 21:51:54 INFO HadoopFSDelegationTokenProvider: getting token for: DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-184728127_1, ugi=dominospark@KDCDOMINO.COM (auth:KERBEROS)]] with renewer yarn/ip-10-0-123-114.us-west-2.compute.internal@KDCDOMINO.COM\n",
      "21/08/13 21:51:55 INFO DFSClient: Created token for dominospark: HDFS_DELEGATION_TOKEN owner=dominospark@KDCDOMINO.COM, renewer=yarn, realUser=, issueDate=1628891515006, maxDate=1629496315006, sequenceNumber=32, masterKeyId=2 on 10.0.123.114:8020\n",
      "21/08/13 21:51:55 INFO KMSClientProvider: New token created: (Kind: kms-dt, Service: kms://http@ip-10-0-123-114.us-west-2.compute.internal:9600/kms, Ident: (kms-dt owner=dominospark, renewer=yarn, realUser=, issueDate=1628891515108, maxDate=1629496315108, sequenceNumber=32, masterKeyId=2))\n",
      "21/08/13 21:51:55 INFO HadoopFSDelegationTokenProvider: getting token for: DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-184728127_1, ugi=dominospark@KDCDOMINO.COM (auth:KERBEROS)]] with renewer dominospark@KDCDOMINO.COM\n",
      "21/08/13 21:51:55 INFO DFSClient: Created token for dominospark: HDFS_DELEGATION_TOKEN owner=dominospark@KDCDOMINO.COM, renewer=dominospark, realUser=, issueDate=1628891515401, maxDate=1629496315401, sequenceNumber=33, masterKeyId=2 on 10.0.123.114:8020\n",
      "21/08/13 21:51:55 INFO KMSClientProvider: New token created: (Kind: kms-dt, Service: kms://http@ip-10-0-123-114.us-west-2.compute.internal:9600/kms, Ident: (kms-dt owner=dominospark, renewer=dominospark, realUser=, issueDate=1628891515417, maxDate=1629496315417, sequenceNumber=33, masterKeyId=2))\n",
      "21/08/13 21:51:55 INFO HadoopFSDelegationTokenProvider: Renewal interval is 86400020 for token kms-dt\n",
      "21/08/13 21:51:55 INFO HadoopFSDelegationTokenProvider: Renewal interval is 86400041 for token HDFS_DELEGATION_TOKEN\n",
      "21/08/13 21:51:57 INFO HiveConf: Found configuration file file:/etc/spark/conf/hive-site.xml\n",
      "21/08/13 21:51:57 WARN HiveConf: HiveConf of name hive.server2.thrift.url does not exist\n",
      "21/08/13 21:51:57 WARN HiveConf: HiveConf of name hive.server2.thrift.url does not exist\n",
      "21/08/13 21:51:58 INFO metastore: Trying to connect to metastore with URI thrift://ip-10-0-123-114.us-west-2.compute.internal:9083\n",
      "21/08/13 21:51:58 INFO metastore: Opened a connection to metastore, current connections: 1\n",
      "21/08/13 21:51:58 INFO metastore: Connected to metastore.\n",
      "21/08/13 21:51:58 INFO metastore: Closed a connection to metastore, current connections: 0\n",
      "21/08/13 21:51:58 INFO Client: Submitting application application_1628875993215_0017 to ResourceManager\n",
      "21/08/13 21:51:58 INFO YarnClientImpl: Submitted application application_1628875993215_0017\n",
      "21/08/13 21:51:59 INFO Client: Application report for application_1628875993215_0017 (state: ACCEPTED)\n",
      "21/08/13 21:51:59 INFO Client: \n",
      "\t client token: Token { kind: YARN_CLIENT_TOKEN, service:  }\n",
      "\t diagnostics: AM container is launched, waiting for AM container to Register with RM\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1628891518523\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-10-0-123-114.us-west-2.compute.internal:20888/proxy/application_1628875993215_0017/\n",
      "\t user: dominospark\n",
      "21/08/13 21:52:00 INFO Client: Application report for application_1628875993215_0017 (state: ACCEPTED)\n",
      "21/08/13 21:52:01 INFO Client: Application report for application_1628875993215_0017 (state: ACCEPTED)\n",
      "21/08/13 21:52:02 INFO Client: Application report for application_1628875993215_0017 (state: ACCEPTED)\n",
      "21/08/13 21:52:03 INFO Client: Application report for application_1628875993215_0017 (state: ACCEPTED)\n",
      "21/08/13 21:52:04 INFO Client: Application report for application_1628875993215_0017 (state: ACCEPTED)\n",
      "21/08/13 21:52:05 INFO Client: Application report for application_1628875993215_0017 (state: ACCEPTED)\n",
      "21/08/13 21:52:06 INFO Client: Application report for application_1628875993215_0017 (state: RUNNING)\n",
      "21/08/13 21:52:06 INFO Client: \n",
      "\t client token: Token { kind: YARN_CLIENT_TOKEN, service:  }\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: ip-10-0-96-11.us-west-2.compute.internal\n",
      "\t ApplicationMaster RPC port: 37373\n",
      "\t queue: default\n",
      "\t start time: 1628891518523\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-10-0-123-114.us-west-2.compute.internal:20888/proxy/application_1628875993215_0017/\n",
      "\t user: dominospark\n",
      "21/08/13 21:52:07 INFO Client: Application report for application_1628875993215_0017 (state: RUNNING)\n",
      "21/08/13 21:52:08 INFO Client: Application report for application_1628875993215_0017 (state: RUNNING)\n",
      "21/08/13 21:52:09 INFO Client: Application report for application_1628875993215_0017 (state: RUNNING)\n",
      "21/08/13 21:52:10 INFO Client: Application report for application_1628875993215_0017 (state: RUNNING)\n",
      "21/08/13 21:52:11 INFO Client: Application report for application_1628875993215_0017 (state: RUNNING)\n",
      "21/08/13 21:52:12 INFO Client: Application report for application_1628875993215_0017 (state: RUNNING)\n",
      "21/08/13 21:52:13 INFO Client: Application report for application_1628875993215_0017 (state: RUNNING)\n",
      "21/08/13 21:52:14 INFO Client: Application report for application_1628875993215_0017 (state: RUNNING)\n",
      "21/08/13 21:52:15 INFO Client: Application report for application_1628875993215_0017 (state: RUNNING)\n",
      "21/08/13 21:52:16 INFO Client: Application report for application_1628875993215_0017 (state: FINISHED)\n",
      "21/08/13 21:52:16 INFO Client: \n",
      "\t client token: Token { kind: YARN_CLIENT_TOKEN, service:  }\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: ip-10-0-96-11.us-west-2.compute.internal\n",
      "\t ApplicationMaster RPC port: 37373\n",
      "\t queue: default\n",
      "\t start time: 1628891518523\n",
      "\t final status: SUCCEEDED\n",
      "\t tracking URL: http://ip-10-0-123-114.us-west-2.compute.internal:20888/proxy/application_1628875993215_0017/\n",
      "\t user: dominospark\n",
      "21/08/13 21:52:16 INFO ShutdownHookManager: Shutdown hook called\n",
      "21/08/13 21:52:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-f5cc1037-c5ce-4c34-99cb-9495047d71a4\n",
      "21/08/13 21:52:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-98427bfa-10b1-4fe4-97c7-794c0c02ef80\n"
     ]
    }
   ],
   "source": [
    "hdfs_endpoint='hdfs://10.0.123.114:8020'\n",
    "!/mnt/code/scripts/my_hdfs.sh dfs -rmr /user/dominospark/smalldata-1000/\n",
    "!/mnt/code/scripts/filter-data-emr.sh \\\n",
    "1000 hdfs://10.0.123.114:8020 /user/dominospark/largedata/ /user/dominospark/smalldata-1000/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /mnt/data/ON-DEMAND-SPARK/smalldata-1000\n",
    "!mkdir /mnt/data/ON-DEMAND-SPARK/smalldata-1000\n",
    "!/mnt/code/scripts/my_hdfs.sh dfs -copyToLocal /user/dominospark/smalldata-1000/* /mnt/data/ON-DEMAND-SPARK/smalldata-1000/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYSPARK_PYTHON=/usr/bin/python3\n",
      "env: PYSPARK_DRIVER_PYTHON=/usr/bin/python3\n"
     ]
    }
   ],
   "source": [
    "%env PYSPARK_PYTHON /usr/bin/python3\n",
    "%env PYSPARK_DRIVER_PYTHON /usr/bin/python3\n",
    "#%env SPARK_DIST_CLASSPATH  /etc/hadoop/conf:/opt/hadoop/share/hadoop/common/lib/*:/opt/hadoop/share/hadoop/common/*:/usr/lib/hadoop-lzo/lib/hadoop-lzo-0.4.19.jar:/usr/lib/hadoop-lzo/lib/hadoop-lzo.jar:/usr/lib/hadoop-lzo/lib/native:/usr/share/aws/aws-java-sdk/aws-java-sdk-bundle-1.11.977.jar:/usr/share/aws/aws-java-sdk/LICENSE.txt:/usr/share/aws/aws-java-sdk/NOTICE.txt:/usr/share/aws/aws-java-sdk/README.md:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/annotations-16.0.2.jar:/usr/share/aws/emr/emrfs/lib/aopalliance-1.0.jar:/usr/share/aws/emr/emrfs/lib/aws-glue-sdk-1.12.0.jar:/usr/share/aws/emr/emrfs/lib/bcprov-ext-jdk15on-1.66.jar:/usr/share/aws/emr/emrfs/lib/emrfs-hadoop-assembly-2.46.0.jar:/usr/share/aws/emr/emrfs/lib/fluent-hc-4.5.6.jar:/usr/share/aws/emr/emrfs/lib/ion-java-1.0.2.jar:/usr/share/aws/emr/emrfs/lib/javax.inject-1.jar:/usr/share/aws/emr/emrfs/lib/jcl-over-slf4j-1.7.21.jar:/usr/share/aws/emr/emrfs/lib/jmespath-java-1.11.852.jar:/usr/share/aws/emr/emrfs/lib/lombok-1.18.4.jar:/usr/share/aws/emr/emrfs/lib/mockito-core-1.10.19.jar:/usr/share/aws/emr/emrfs/lib/objenesis-2.1.jar:/usr/share/aws/emr/emrfs/lib/secret-agent-interface-1.4.0.jar:/usr/share/aws/emr/emrfs/lib/slf4j-api-1.7.21.jar:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/ddb/lib/emr-ddb-hadoop.jar:/usr/share/aws/emr/goodies/lib/emr-hadoop-goodies.jar:/usr/share/aws/emr/kinesis/lib/emr-kinesis-hadoop.jar:/usr/share/aws/emr/cloudwatch-sink/lib/cloudwatch-sink-2.1.0.jar:/usr/share/aws/emr/cloudwatch-sink/lib/cloudwatch-sink.jar:/opt/hadoop/share/hadoop/tools/lib/*:/usr/lib/hadoop-lzo/lib/*\n",
    "#%env JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, DoubleType, IntegerType\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession = SparkSession.builder.appName(\"ML On Smaller Datasets Copied from HDFS\") \\\n",
    ".config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    ".config(\"spark.driver.extraClassPath\", \"/opt/hadoop/etc/hadoop:/usr/lib/hadoop-lzo/lib/*:/opt/hadoop/share/hadoop/mapreduce/*:/opt/hadoop/share/hadoop/hdfs/*\") \\\n",
    ".getOrCreate()\n",
    "sc=sparkSession.sparkContext\n",
    "#.config(\"spark.executor.extraClassPath\", \"/opt/hadoop/etc/hadoop:/usr/lib/hadoop-lzo/lib/*:/opt/hadoop/share/hadoop/mapreduce/*:/opt/hadoop/share/hadoop/hdfs/*\") \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "| id| v1| v2| v3|\n",
      "+---+---+---+---+\n",
      "|  0| 56| 36| 74|\n",
      "|  1| 82| 58| 55|\n",
      "|  2| 28| 54| 64|\n",
      "|  3| 50| 99| 72|\n",
      "|  4| 45|  9| 13|\n",
      "|  5| 11| 86| 37|\n",
      "|  6| 57| 59| 67|\n",
      "|  7| 76| 48| 42|\n",
      "|  8| 19|  5| 85|\n",
      "|  9| 96| 29| 26|\n",
      "| 10| 83| 99| 79|\n",
      "| 11| 64| 23|  6|\n",
      "| 12| 53| 96| 99|\n",
      "| 13| 46| 91| 55|\n",
      "| 14| 24| 84| 71|\n",
      "| 15| 53|  4| 79|\n",
      "| 16|  2| 60| 43|\n",
      "| 17| 38| 89| 99|\n",
      "| 18| 10| 62| 48|\n",
      "| 19| 36| 54| 79|\n",
      "+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = StructType([ StructField(\"id\", IntegerType(), True),\n",
    "                       StructField(\"v1\", IntegerType(), True),\n",
    "                       StructField(\"v2\", IntegerType(), True),\n",
    "                       StructField(\"v3\", IntegerType(), True) ])\n",
    "df_load = sparkSession.read.csv('file:///mnt/data/ON_DEMAND_SPARK/smalldata-1000',schema=columns)\n",
    "df_load.show()\n",
    "df_load.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 items\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-13 18:02 .sparkStaging\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-12 22:06 example\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-13 16:23 largedata\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-13 14:14 mypi\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-13 15:32 small_data\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-13 16:27 smalldata\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-13 18:02 smalldata-1000\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-11 15:36 test\n"
     ]
    }
   ],
   "source": [
    "!/mnt/code/scripts/my_hdfs.sh dfs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
