{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use '-rm -r' instead.\n",
      "Deleted /user/dominospark/smalldata-1000\n",
      "rmr: DEPRECATED: Please use '-rm -r' instead.\n",
      "rmr: `/user/dominospark/smalldata-1000/': No such file or directory\n",
      "log4j:ERROR setFile(null,true) call failed.\n",
      "java.io.FileNotFoundException: /var/log/spark/user/ubuntu/stderr (No such file or directory)\n",
      "\tat java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\n",
      "\tat java.io.FileOutputStream.<init>(FileOutputStream.java:213)\n",
      "\tat java.io.FileOutputStream.<init>(FileOutputStream.java:133)\n",
      "\tat org.apache.log4j.FileAppender.setFile(FileAppender.java:294)\n",
      "\tat org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165)\n",
      "\tat org.apache.log4j.DailyRollingFileAppender.activateOptions(DailyRollingFileAppender.java:223)\n",
      "\tat org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307)\n",
      "\tat org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:172)\n",
      "\tat org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:104)\n",
      "\tat org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:842)\n",
      "\tat org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768)\n",
      "\tat org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:672)\n",
      "\tat org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:516)\n",
      "\tat org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:580)\n",
      "\tat org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)\n",
      "\tat org.apache.log4j.LogManager.<clinit>(LogManager.java:127)\n",
      "\tat org.slf4j.impl.Log4jLoggerFactory.<init>(Log4jLoggerFactory.java:66)\n",
      "\tat org.slf4j.impl.StaticLoggerBinder.<init>(StaticLoggerBinder.java:72)\n",
      "\tat org.slf4j.impl.StaticLoggerBinder.<clinit>(StaticLoggerBinder.java:45)\n",
      "\tat org.apache.spark.internal.Logging$.org$apache$spark$internal$Logging$$isLog4j12(Logging.scala:222)\n",
      "\tat org.apache.spark.internal.Logging.initializeLogging(Logging.scala:127)\n",
      "\tat org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:111)\n",
      "\tat org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:105)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.initializeLogIfNecessary(SparkSubmit.scala:75)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:83)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1038)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1047)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "log4j:ERROR Either File or DatePattern options are not set for appender [DRFA-stderr].\n",
      "log4j:ERROR setFile(null,true) call failed.\n",
      "java.io.FileNotFoundException: /var/log/spark/user/ubuntu/stdout (No such file or directory)\n",
      "\tat java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\n",
      "\tat java.io.FileOutputStream.<init>(FileOutputStream.java:213)\n",
      "\tat java.io.FileOutputStream.<init>(FileOutputStream.java:133)\n",
      "\tat org.apache.log4j.FileAppender.setFile(FileAppender.java:294)\n",
      "\tat org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165)\n",
      "\tat org.apache.log4j.DailyRollingFileAppender.activateOptions(DailyRollingFileAppender.java:223)\n",
      "\tat org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307)\n",
      "\tat org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:172)\n",
      "\tat org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:104)\n",
      "\tat org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:842)\n",
      "\tat org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768)\n",
      "\tat org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:672)\n",
      "\tat org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:516)\n",
      "\tat org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:580)\n",
      "\tat org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)\n",
      "\tat org.apache.log4j.LogManager.<clinit>(LogManager.java:127)\n",
      "\tat org.slf4j.impl.Log4jLoggerFactory.<init>(Log4jLoggerFactory.java:66)\n",
      "\tat org.slf4j.impl.StaticLoggerBinder.<init>(StaticLoggerBinder.java:72)\n",
      "\tat org.slf4j.impl.StaticLoggerBinder.<clinit>(StaticLoggerBinder.java:45)\n",
      "\tat org.apache.spark.internal.Logging$.org$apache$spark$internal$Logging$$isLog4j12(Logging.scala:222)\n",
      "\tat org.apache.spark.internal.Logging.initializeLogging(Logging.scala:127)\n",
      "\tat org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:111)\n",
      "\tat org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:105)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.initializeLogIfNecessary(SparkSubmit.scala:75)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:83)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1038)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1047)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "log4j:ERROR Either File or DatePattern options are not set for appender [DRFA-stdout].\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "21/08/14 23:45:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "21/08/14 23:45:26 INFO RMProxy: Connecting to ResourceManager at ip-10-0-123-114.us-west-2.compute.internal/10.0.123.114:8032\n",
      "21/08/14 23:45:26 INFO Client: Requesting a new application from cluster with 2 NodeManagers\n",
      "21/08/14 23:45:27 INFO Configuration: resource-types.xml not found\n",
      "21/08/14 23:45:27 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "21/08/14 23:45:27 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)\n",
      "21/08/14 23:45:27 INFO Client: Will allocate AM container, with 2432 MB memory including 384 MB overhead\n",
      "21/08/14 23:45:27 INFO Client: Setting up container launch context for our AM\n",
      "21/08/14 23:45:27 INFO Client: Setting up the launch environment for our AM container\n",
      "21/08/14 23:45:27 INFO Client: Preparing resources for our AM container\n",
      "21/08/14 23:45:27 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "21/08/14 23:45:29 INFO Client: Uploading resource file:/tmp/spark-612afb15-4d5f-488d-a033-6423287f5a69/__spark_libs__3224194779272108614.zip -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0023/__spark_libs__3224194779272108614.zip\n",
      "21/08/14 23:45:31 INFO Client: Uploading resource file:/etc/spark/conf/hive-site.xml -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0023/hive-site.xml\n",
      "21/08/14 23:45:31 INFO Client: Uploading resource file:/mnt/code/python/filter_data_in_hdfs.py -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0023/filter_data_in_hdfs.py\n",
      "21/08/14 23:45:31 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0023/pyspark.zip\n",
      "21/08/14 23:45:31 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9-src.zip -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0023/py4j-0.10.9-src.zip\n",
      "21/08/14 23:45:32 INFO Client: Uploading resource file:/tmp/spark-612afb15-4d5f-488d-a033-6423287f5a69/__spark_conf__5730224728456950324.zip -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0023/__spark_conf__.zip\n",
      "21/08/14 23:45:32 INFO SecurityManager: Changing view acls to: ubuntu,dominospark\n",
      "21/08/14 23:45:32 INFO SecurityManager: Changing modify acls to: ubuntu,dominospark\n",
      "21/08/14 23:45:32 INFO SecurityManager: Changing view acls groups to: \n",
      "21/08/14 23:45:32 INFO SecurityManager: Changing modify acls groups to: \n",
      "21/08/14 23:45:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ubuntu, dominospark); groups with view permissions: Set(); users  with modify permissions: Set(ubuntu, dominospark); groups with modify permissions: Set()\n",
      "21/08/14 23:45:32 INFO HadoopDelegationTokenManager: Attempting to load user's ticket cache.\n",
      "21/08/14 23:45:32 INFO HadoopFSDelegationTokenProvider: getting token for: DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1467772237_1, ugi=dominospark@KDCDOMINO.COM (auth:KERBEROS)]] with renewer yarn/ip-10-0-123-114.us-west-2.compute.internal@KDCDOMINO.COM\n",
      "21/08/14 23:45:32 INFO DFSClient: Created token for dominospark: HDFS_DELEGATION_TOKEN owner=dominospark@KDCDOMINO.COM, renewer=yarn, realUser=, issueDate=1628984732504, maxDate=1629589532504, sequenceNumber=59, masterKeyId=3 on 10.0.123.114:8020\n",
      "21/08/14 23:45:32 INFO KMSClientProvider: New token created: (Kind: kms-dt, Service: kms://http@ip-10-0-123-114.us-west-2.compute.internal:9600/kms, Ident: (kms-dt owner=dominospark, renewer=yarn, realUser=, issueDate=1628984732626, maxDate=1629589532626, sequenceNumber=40, masterKeyId=3))\n",
      "21/08/14 23:45:32 INFO HadoopFSDelegationTokenProvider: getting token for: DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1467772237_1, ugi=dominospark@KDCDOMINO.COM (auth:KERBEROS)]] with renewer dominospark@KDCDOMINO.COM\n",
      "21/08/14 23:45:32 INFO DFSClient: Created token for dominospark: HDFS_DELEGATION_TOKEN owner=dominospark@KDCDOMINO.COM, renewer=dominospark, realUser=, issueDate=1628984732907, maxDate=1629589532907, sequenceNumber=60, masterKeyId=3 on 10.0.123.114:8020\n",
      "21/08/14 23:45:32 INFO KMSClientProvider: New token created: (Kind: kms-dt, Service: kms://http@ip-10-0-123-114.us-west-2.compute.internal:9600/kms, Ident: (kms-dt owner=dominospark, renewer=dominospark, realUser=, issueDate=1628984732976, maxDate=1629589532976, sequenceNumber=41, masterKeyId=3))\n",
      "21/08/14 23:45:33 INFO HadoopFSDelegationTokenProvider: Renewal interval is 86400024 for token kms-dt\n",
      "21/08/14 23:45:33 INFO HadoopFSDelegationTokenProvider: Renewal interval is 86400099 for token HDFS_DELEGATION_TOKEN\n",
      "21/08/14 23:45:35 INFO HiveConf: Found configuration file file:/etc/spark/conf/hive-site.xml\n",
      "21/08/14 23:45:35 WARN HiveConf: HiveConf of name hive.server2.thrift.url does not exist\n",
      "21/08/14 23:45:35 WARN HiveConf: HiveConf of name hive.server2.thrift.url does not exist\n",
      "21/08/14 23:45:36 INFO metastore: Trying to connect to metastore with URI thrift://ip-10-0-123-114.us-west-2.compute.internal:9083\n",
      "21/08/14 23:45:36 INFO metastore: Opened a connection to metastore, current connections: 1\n",
      "21/08/14 23:45:36 INFO metastore: Connected to metastore.\n",
      "21/08/14 23:45:36 INFO metastore: Closed a connection to metastore, current connections: 0\n",
      "21/08/14 23:45:36 INFO Client: Submitting application application_1628875993215_0023 to ResourceManager\n",
      "21/08/14 23:45:36 INFO YarnClientImpl: Submitted application application_1628875993215_0023\n",
      "21/08/14 23:45:37 INFO Client: Application report for application_1628875993215_0023 (state: ACCEPTED)\n",
      "21/08/14 23:45:37 INFO Client: \n",
      "\t client token: Token { kind: YARN_CLIENT_TOKEN, service:  }\n",
      "\t diagnostics: AM container is launched, waiting for AM container to Register with RM\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1628984736389\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-10-0-123-114.us-west-2.compute.internal:20888/proxy/application_1628875993215_0023/\n",
      "\t user: dominospark\n",
      "21/08/14 23:45:38 INFO Client: Application report for application_1628875993215_0023 (state: ACCEPTED)\n",
      "21/08/14 23:45:39 INFO Client: Application report for application_1628875993215_0023 (state: ACCEPTED)\n",
      "21/08/14 23:45:40 INFO Client: Application report for application_1628875993215_0023 (state: ACCEPTED)\n",
      "21/08/14 23:45:41 INFO Client: Application report for application_1628875993215_0023 (state: ACCEPTED)\n",
      "21/08/14 23:45:42 INFO Client: Application report for application_1628875993215_0023 (state: ACCEPTED)\n",
      "21/08/14 23:45:43 INFO Client: Application report for application_1628875993215_0023 (state: ACCEPTED)\n",
      "21/08/14 23:45:44 INFO Client: Application report for application_1628875993215_0023 (state: RUNNING)\n",
      "21/08/14 23:45:44 INFO Client: \n",
      "\t client token: Token { kind: YARN_CLIENT_TOKEN, service:  }\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: ip-10-0-113-154.us-west-2.compute.internal\n",
      "\t ApplicationMaster RPC port: 45413\n",
      "\t queue: default\n",
      "\t start time: 1628984736389\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-10-0-123-114.us-west-2.compute.internal:20888/proxy/application_1628875993215_0023/\n",
      "\t user: dominospark\n",
      "21/08/14 23:45:45 INFO Client: Application report for application_1628875993215_0023 (state: RUNNING)\n",
      "21/08/14 23:45:46 INFO Client: Application report for application_1628875993215_0023 (state: RUNNING)\n",
      "21/08/14 23:45:47 INFO Client: Application report for application_1628875993215_0023 (state: RUNNING)\n",
      "21/08/14 23:45:48 INFO Client: Application report for application_1628875993215_0023 (state: RUNNING)\n",
      "21/08/14 23:45:49 INFO Client: Application report for application_1628875993215_0023 (state: RUNNING)\n",
      "21/08/14 23:45:50 INFO Client: Application report for application_1628875993215_0023 (state: RUNNING)\n",
      "21/08/14 23:45:51 INFO Client: Application report for application_1628875993215_0023 (state: RUNNING)\n",
      "21/08/14 23:45:52 INFO Client: Application report for application_1628875993215_0023 (state: RUNNING)\n",
      "21/08/14 23:45:53 INFO Client: Application report for application_1628875993215_0023 (state: RUNNING)\n",
      "21/08/14 23:45:54 INFO Client: Application report for application_1628875993215_0023 (state: FINISHED)\n",
      "21/08/14 23:45:54 INFO Client: \n",
      "\t client token: Token { kind: YARN_CLIENT_TOKEN, service:  }\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: ip-10-0-113-154.us-west-2.compute.internal\n",
      "\t ApplicationMaster RPC port: 45413\n",
      "\t queue: default\n",
      "\t start time: 1628984736389\n",
      "\t final status: SUCCEEDED\n",
      "\t tracking URL: http://ip-10-0-123-114.us-west-2.compute.internal:20888/proxy/application_1628875993215_0023/\n",
      "\t user: dominospark\n",
      "21/08/14 23:45:54 INFO ShutdownHookManager: Shutdown hook called\n",
      "21/08/14 23:45:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-612afb15-4d5f-488d-a033-6423287f5a69\n",
      "21/08/14 23:45:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-c5646e60-48c5-4034-9e65-2aeb31205877\n"
     ]
    }
   ],
   "source": [
    "## Using ON DEMAND SPARK without HDFS Delegation Token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "hdfs_endpoint=os.environ['HDFS_ENDPOINT']\n",
    "!/mnt/code/scripts/my_hdfs.sh dfs -rmr /user/dominospark/smalldata-1000/\n",
    "!/mnt/code/scripts/filter-data-emr.sh \\\n",
    "1000 hdfs://10.0.123.114:8020 /user/dominospark/largedata/ /user/dominospark/smalldata-1000/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /mnt/data/ON-DEMAND-SPARK/smalldata-1000\n",
    "!mkdir /mnt/data/ON-DEMAND-SPARK/smalldata-1000\n",
    "!/mnt/code/scripts/my_hdfs.sh dfs -copyToLocal /user/dominospark/smalldata-1000/* /mnt/data/ON-DEMAND-SPARK/smalldata-1000/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYSPARK_PYTHON=/usr/bin/python3\n",
      "env: PYSPARK_DRIVER_PYTHON=/usr/bin/python3\n"
     ]
    }
   ],
   "source": [
    "%env PYSPARK_PYTHON /usr/bin/python3\n",
    "%env PYSPARK_DRIVER_PYTHON /usr/bin/python3\n",
    "#%env SPARK_DIST_CLASSPATH  /etc/hadoop/conf:/opt/hadoop/share/hadoop/common/lib/*:/opt/hadoop/share/hadoop/common/*:/usr/lib/hadoop-lzo/lib/hadoop-lzo-0.4.19.jar:/usr/lib/hadoop-lzo/lib/hadoop-lzo.jar:/usr/lib/hadoop-lzo/lib/native:/usr/share/aws/aws-java-sdk/aws-java-sdk-bundle-1.11.977.jar:/usr/share/aws/aws-java-sdk/LICENSE.txt:/usr/share/aws/aws-java-sdk/NOTICE.txt:/usr/share/aws/aws-java-sdk/README.md:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/annotations-16.0.2.jar:/usr/share/aws/emr/emrfs/lib/aopalliance-1.0.jar:/usr/share/aws/emr/emrfs/lib/aws-glue-sdk-1.12.0.jar:/usr/share/aws/emr/emrfs/lib/bcprov-ext-jdk15on-1.66.jar:/usr/share/aws/emr/emrfs/lib/emrfs-hadoop-assembly-2.46.0.jar:/usr/share/aws/emr/emrfs/lib/fluent-hc-4.5.6.jar:/usr/share/aws/emr/emrfs/lib/ion-java-1.0.2.jar:/usr/share/aws/emr/emrfs/lib/javax.inject-1.jar:/usr/share/aws/emr/emrfs/lib/jcl-over-slf4j-1.7.21.jar:/usr/share/aws/emr/emrfs/lib/jmespath-java-1.11.852.jar:/usr/share/aws/emr/emrfs/lib/lombok-1.18.4.jar:/usr/share/aws/emr/emrfs/lib/mockito-core-1.10.19.jar:/usr/share/aws/emr/emrfs/lib/objenesis-2.1.jar:/usr/share/aws/emr/emrfs/lib/secret-agent-interface-1.4.0.jar:/usr/share/aws/emr/emrfs/lib/slf4j-api-1.7.21.jar:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/ddb/lib/emr-ddb-hadoop.jar:/usr/share/aws/emr/goodies/lib/emr-hadoop-goodies.jar:/usr/share/aws/emr/kinesis/lib/emr-kinesis-hadoop.jar:/usr/share/aws/emr/cloudwatch-sink/lib/cloudwatch-sink-2.1.0.jar:/usr/share/aws/emr/cloudwatch-sink/lib/cloudwatch-sink.jar:/opt/hadoop/share/hadoop/tools/lib/*:/usr/lib/hadoop-lzo/lib/*\n",
    "#%env JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, DoubleType, IntegerType\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession = SparkSession.builder.appName(\"ML On Smaller Datasets Copied from HDFS\") \\\n",
    ".config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    ".config(\"spark.driver.extraClassPath\", \"/opt/hadoop/etc/hadoop:/usr/lib/hadoop-lzo/lib/*:/opt/hadoop/share/hadoop/mapreduce/*:/opt/hadoop/share/hadoop/hdfs/*\") \\\n",
    ".getOrCreate()\n",
    "sc=sparkSession.sparkContext\n",
    "#.config(\"spark.executor.extraClassPath\", \"/opt/hadoop/etc/hadoop:/usr/lib/hadoop-lzo/lib/*:/opt/hadoop/share/hadoop/mapreduce/*:/opt/hadoop/share/hadoop/hdfs/*\") \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "| id| v1| v2| v3|\n",
      "+---+---+---+---+\n",
      "|  0| 63| 94| 50|\n",
      "|  1| 25| 26| 73|\n",
      "|  2| 84| 84| 84|\n",
      "|  3| 47| 19|  8|\n",
      "|  4| 24| 31|  6|\n",
      "|  5| 75| 17| 11|\n",
      "|  6| 49| 38| 57|\n",
      "|  7| 56| 31| 90|\n",
      "|  8|100|  3|  3|\n",
      "|  9| 43| 72| 34|\n",
      "| 10| 18| 64| 57|\n",
      "| 11| 63| 75| 80|\n",
      "| 12| 82| 85| 28|\n",
      "| 13| 31|  8| 42|\n",
      "| 14| 20| 80|  3|\n",
      "| 15| 27| 91| 86|\n",
      "| 16| 55| 70| 42|\n",
      "| 17| 69|  3|  5|\n",
      "| 18| 65| 28| 28|\n",
      "| 19| 57|  8| 69|\n",
      "+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = StructType([ StructField(\"id\", IntegerType(), True),\n",
    "                       StructField(\"v1\", IntegerType(), True),\n",
    "                       StructField(\"v2\", IntegerType(), True),\n",
    "                       StructField(\"v3\", IntegerType(), True) ])\n",
    "df_load = sparkSession.read.csv('file:///mnt/data/ON-DEMAND-SPARK/smalldata-1000',schema=columns)\n",
    "df_load.show()\n",
    "df_load.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 items\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-14 18:53 .sparkStaging\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-13 20:45 example\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-13 21:45 large-data\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-13 21:51 largedata\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-13 21:57 ld-10\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-13 21:07 mypi\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-13 21:58 sd-5\n",
      "drwxr-xr-x   - dominospark dominospark          0 2021-08-14 18:53 smalldata-1000\n"
     ]
    }
   ],
   "source": [
    "!/mnt/code/scripts/my_hdfs.sh dfs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
