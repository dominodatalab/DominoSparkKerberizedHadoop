{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pprint, requests, textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sparkmagic.magics extension is already loaded. To reload it, use:\n",
      "  %reload_ext sparkmagic.magics\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext sparkmagic.magics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = 'http://10.0.123.114:8998'\n",
    "data = {'kind': 'pyspark'}\n",
    "json.dumps(data)\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First element of numbers is 1 and its description is:\n",
      "b'(2) ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274 []'"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "numbers = sc.parallelize([1, 2, 3, 4])\n",
    "print('First element of numbers is {} and its description is:\\n{}'.format(numbers.first(), numbers.toDebugString()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stdout: \n",
      "\n",
      "stderr: \n",
      "21/08/16 20:35:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "21/08/16 20:35:30 INFO RMProxy: Connecting to ResourceManager at ip-10-0-123-114.us-west-2.compute.internal/10.0.123.114:8032\n",
      "21/08/16 20:35:30 INFO Client: Requesting a new application from cluster with 2 NodeManagers\n",
      "21/08/16 20:35:31 INFO Configuration: resource-types.xml not found\n",
      "21/08/16 20:35:31 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "21/08/16 20:35:31 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)\n",
      "21/08/16 20:35:31 INFO Client: Will allocate AM container, with 1384 MB memory including 384 MB overhead\n",
      "21/08/16 20:35:31 INFO Client: Setting up container launch context for our AM\n",
      "21/08/16 20:35:31 INFO Client: Setting up the launch environment for our AM container\n",
      "21/08/16 20:35:31 INFO Client: Preparing resources for our AM container\n",
      "21/08/16 20:35:31 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "21/08/16 20:35:33 INFO Client: Uploading resource file:/mnt/tmp/spark-7e7df9d4-e67f-4484-a0b7-941c8192f876/__spark_libs__539692263725692300.zip -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0024/__spark_libs__539692263725692300.zip\n",
      "21/08/16 20:35:34 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-api-0.7.0-incubating.jar -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0024/livy-api-0.7.0-incubating.jar\n",
      "21/08/16 20:35:34 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-rsc-0.7.0-incubating.jar -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0024/livy-rsc-0.7.0-incubating.jar\n",
      "21/08/16 20:35:34 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-thriftserver-session-0.7.0-incubating.jar -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0024/livy-thriftserver-session-0.7.0-incubating.jar\n",
      "21/08/16 20:35:34 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0024/netty-all-4.1.17.Final.jar\n",
      "21/08/16 20:35:34 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/commons-codec-1.9.jar -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0024/commons-codec-1.9.jar\n",
      "21/08/16 20:35:34 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/livy-core_2.12-0.7.0-incubating.jar -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0024/livy-core_2.12-0.7.0-incubating.jar\n",
      "21/08/16 20:35:34 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/livy-repl_2.12-0.7.0-incubating.jar -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0024/livy-repl_2.12-0.7.0-incubating.jar\n",
      "21/08/16 20:35:34 INFO Client: Uploading resource file:/etc/spark/conf/hive-site.xml -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0024/hive-site.xml\n",
      "21/08/16 20:35:34 INFO Client: Uploading resource file:/usr/lib/spark/R/lib/sparkr.zip#sparkr -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0024/sparkr.zip\n",
      "21/08/16 20:35:34 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0024/pyspark.zip\n",
      "21/08/16 20:35:34 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9-src.zip -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0024/py4j-0.10.9-src.zip\n",
      "21/08/16 20:35:35 INFO Client: Uploading resource file:/mnt/tmp/spark-7e7df9d4-e67f-4484-a0b7-941c8192f876/__spark_conf__9090865076616500563.zip -> hdfs://ip-10-0-123-114.us-west-2.compute.internal:8020/user/dominospark/.sparkStaging/application_1628875993215_0024/__spark_conf__.zip\n",
      "21/08/16 20:35:35 INFO SecurityManager: Changing view acls to: livy,dominospark\n",
      "21/08/16 20:35:35 INFO SecurityManager: Changing modify acls to: livy,dominospark\n",
      "21/08/16 20:35:35 INFO SecurityManager: Changing view acls groups to: \n",
      "21/08/16 20:35:35 INFO SecurityManager: Changing modify acls groups to: \n",
      "21/08/16 20:35:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, dominospark); groups with view permissions: Set(); users  with modify permissions: Set(livy, dominospark); groups with modify permissions: Set()\n",
      "21/08/16 20:35:35 INFO HadoopFSDelegationTokenProvider: getting token for: DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1592515283_1, ugi=dominospark (auth:PROXY) via livy/ip-10-0-123-114.us-west-2.compute.internal@KDCDOMINO.COM (auth:KERBEROS)]] with renewer yarn/ip-10-0-123-114.us-west-2.compute.internal@KDCDOMINO.COM\n",
      "21/08/16 20:35:35 INFO DFSClient: Created token for dominospark: HDFS_DELEGATION_TOKEN owner=dominospark, renewer=yarn, realUser=livy/ip-10-0-123-114.us-west-2.compute.internal@KDCDOMINO.COM, issueDate=1629146135212, maxDate=1629750935212, sequenceNumber=69, masterKeyId=5 on 10.0.123.114:8020\n",
      "21/08/16 20:35:35 INFO KMSClientProvider: New token created: (Kind: kms-dt, Service: kms://http@ip-10-0-123-114.us-west-2.compute.internal:9600/kms, Ident: (kms-dt owner=dominospark, renewer=yarn, realUser=livy, issueDate=1629146135279, maxDate=1629750935279, sequenceNumber=42, masterKeyId=5))\n",
      "21/08/16 20:35:35 INFO HadoopFSDelegationTokenProvider: getting token for: DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1592515283_1, ugi=dominospark (auth:PROXY) via livy/ip-10-0-123-114.us-west-2.compute.internal@KDCDOMINO.COM (auth:KERBEROS)]] with renewer dominospark\n",
      "21/08/16 20:35:35 INFO DFSClient: Created token for dominospark: HDFS_DELEGATION_TOKEN owner=dominospark, renewer=dominospark, realUser=livy/ip-10-0-123-114.us-west-2.compute.internal@KDCDOMINO.COM, issueDate=1629146135477, maxDate=1629750935477, sequenceNumber=70, masterKeyId=5 on 10.0.123.114:8020\n",
      "21/08/16 20:35:35 INFO KMSClientProvider: New token created: (Kind: kms-dt, Service: kms://http@ip-10-0-123-114.us-west-2.compute.internal:9600/kms, Ident: (kms-dt owner=dominospark, renewer=dominospark, realUser=livy, issueDate=1629146135494, maxDate=1629750935494, sequenceNumber=43, masterKeyId=5))\n",
      "21/08/16 20:35:35 INFO HadoopFSDelegationTokenProvider: Renewal interval is 86400024 for token kms-dt\n",
      "21/08/16 20:35:35 INFO HadoopFSDelegationTokenProvider: Renewal interval is 86400047 for token HDFS_DELEGATION_TOKEN\n",
      "21/08/16 20:35:36 INFO HiveConf: Found configuration file file:/etc/spark/conf.dist/hive-site.xml\n",
      "21/08/16 20:35:36 WARN HiveConf: HiveConf of name hive.server2.thrift.url does not exist\n",
      "21/08/16 20:35:36 WARN HiveConf: HiveConf of name hive.server2.thrift.url does not exist\n",
      "21/08/16 20:35:37 INFO metastore: Trying to connect to metastore with URI thrift://ip-10-0-123-114.us-west-2.compute.internal:9083\n",
      "21/08/16 20:35:37 INFO metastore: Opened a connection to metastore, current connections: 1\n",
      "21/08/16 20:35:37 INFO metastore: Connected to metastore.\n",
      "21/08/16 20:35:37 INFO metastore: Closed a connection to metastore, current connections: 0\n",
      "21/08/16 20:35:37 INFO Client: Submitting application application_1628875993215_0024 to ResourceManager\n",
      "21/08/16 20:35:37 INFO YarnClientImpl: Submitted application application_1628875993215_0024\n",
      "21/08/16 20:35:37 INFO Client: Application report for application_1628875993215_0024 (state: ACCEPTED)\n",
      "21/08/16 20:35:37 INFO Client: \n",
      "\t client token: Token { kind: YARN_CLIENT_TOKEN, service:  }\n",
      "\t diagnostics: [Mon Aug 16 20:35:37 +0000 2021] Scheduler has assigned a container for AM, waiting for AM container to be launched\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1629146137370\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-10-0-123-114.us-west-2.compute.internal:20888/proxy/application_1628875993215_0024/\n",
      "\t user: dominospark\n",
      "21/08/16 20:35:37 INFO ShutdownHookManager: Shutdown hook called\n",
      "21/08/16 20:35:37 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-58a6382b-a4fb-4cee-9e54-e9d0c1fffa6d\n",
      "21/08/16 20:35:37 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-7e7df9d4-e67f-4484-a0b7-941c8192f876\n",
      "\n",
      "YARN Diagnostics: "
     ]
    }
   ],
   "source": [
    "%spark logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers)\n",
    "json.dumps(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_url = host + r.headers['location']\n",
    "r = requests.get(session_url, headers=headers)\n",
    "r.json()\n",
    "\n",
    "{u'state': u'idle', u'id': 0, u'kind': u'spark'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "  'code': textwrap.dedent(\"\"\"\n",
    "    import random\n",
    "    NUM_SAMPLES = 100000\n",
    "    def sample(p):\n",
    "      x, y = random.random(), random.random()\n",
    "      return 1 if x*x + y*y < 1 else 0\n",
    "\n",
    "    count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b)\n",
    "    print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)\n",
    "    \"\"\")\n",
    "}\n",
    "\n",
    "r = requests.post(statements_url, data=json.dumps(data), headers=headers)\n",
    "pprint.pprint(r.json())\n",
    "\n",
    "{u'id': 12,\n",
    "u'output': {u'data': {u'text/plain': u'Pi is roughly 3.136000'},\n",
    "            u'execution_count': 12,\n",
    "            u'status': u'ok'},\n",
    "u'state': u'running'}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
